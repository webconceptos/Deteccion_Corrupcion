"""
09_generar_reporte_sprint4.py
-----------------------------
Generador automático del informe técnico del Sprint 4.

Este script consolida:

- Métricas del modelo BASELINE
- Métricas del MODELO ACTUAL
- Comparación de métricas
- Latencia CPU (sklearn)
- Latencia ONNX (onnxruntime)
- Latencia TensorRT (si existe)
- Resultados de percepción de usuarios (encuesta)
- Notas sobre pruebas de carga (Locust), si se dispone de datos

Salida principal:
- models/sprint4/reportes/informe_sprint4.md

Este archivo es el "corazón" del entregable Sprint 4:
deja un informe reproducible, en texto plano, fácil de versionar en Git.

Autor: Fer + Jarvis
"""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Dict, Any, Optional

import pandas as pd

from utils_sprint4 import (
    setup_logger,
    ensure_parent_dir,
)


# ============================================================
# Helpers
# ============================================================

def safe_read_csv(path: Path) -> Optional[pd.DataFrame]:
    """Lee un CSV si existe, en caso contrario retorna None."""
    if not path.exists():
        return None
    return pd.read_csv(path)


def df_first_row_as_dict(df: Optional[pd.DataFrame]) -> Dict[str, Any]:
    """Convierte la primera fila de un DataFrame en dict (o dict vacío si df es None)."""
    if df is None or df.empty:
        return {}
    return df.iloc[0].to_dict()


def fmt_float(value: Any, ndigits: int = 4) -> str:
    """Formatea flotantes con ndigits decimales, deja otros valores tal cual."""
    try:
        return f"{float(value):.{ndigits}f}"
    except Exception:
        return str(value)


def section_title(text: str, level: int = 2) -> str:
    """Genera títulos markdown con #."""
    return f"\n{'#' * level} {text}\n"


# ============================================================
# Construcción del informe
# ============================================================

def build_report_markdown(
    metrics_baseline: Dict[str, Any],
    metrics_model: Dict[str, Any],
    latency_cpu: Dict[str, Any],
    latency_onnx: Dict[str, Any],
    latency_trt: Dict[str, Any],
    perception: Dict[str, Any],
) -> str:
    """
    Construye el texto completo del informe Sprint 4 en Markdown.
    """

    # ===============================
    # 1. Portada / Introducción
    # ===============================
    md = []
    md.append("# Informe Sprint 4 – Evaluación de Modelo y Despliegue\n")
    md.append("**Proyecto:** Detección de riesgos de corrupción en obras públicas\n")
    md.append("**Sprint:** 4 – Validación técnica, operativa y de usuario\n")

    md.append(
        "\nEste informe consolida la evidencia generada durante el Sprint 4, "
        "incluyendo comparación de modelos, análisis de latencia, percepción "
        "de usuarios y consideraciones para el despliegue.\n"
    )

    # ===============================
    # 2. Comparación de modelos
    # ===============================
    md.append(section_title("Comparación de desempeño: Baseline vs Modelo Actual"))

    if metrics_baseline and metrics_model:
        md.append(
            "Se comparan las métricas clave de clasificación entre el modelo "
            "baseline (referencia) y el modelo actual optimizado.\n"
        )

        keys = sorted(set(metrics_baseline.keys()) | set(metrics_model.keys()))
        md.append("| Métrica | Baseline | Modelo Actual |\n")
        md.append("|--------|----------|---------------|\n")
        for k in keys:
            v_b = fmt_float(metrics_baseline.get(k, "N/A"))
            v_m = fmt_float(metrics_model.get(k, "N/A"))
            md.append(f"| {k} | {v_b} | {v_m} |\n")

        md.append("\n**Interpretación:**\n")
        md.append(
            "- Se espera que el modelo actual muestre mejoras en F1, recall y precisión "
            "sobre el baseline.\n"
        )
        md.append(
            "- Estas métricas deben analizarse especialmente en función de la clase "
            "de mayor interés (ej. obras con alto riesgo de corrupción).\n"
        )
    else:
        md.append(
            "No se encontraron métricas completas para ambos modelos. "
            "Verificar archivos de resultados en `models/sprint4/resultados/`.\n"
        )

    # ===============================
    # 3. Latencia y rendimiento
    # ===============================
    md.append(section_title("Análisis de latencia y rendimiento"))

    md.append(
        "Se evaluó la latencia de inferencia bajo distintos escenarios de ejecución:\n"
        "- CPU con modelo sklearn\n"
        "- ONNX Runtime (CPU optimizado)\n"
        "- TensorRT (GPU, si se dispone)\n"
    )

    def latency_table_section(title: str, stats: Dict[str, Any]) -> str:
        if not stats:
            return f"\n_No se encontraron resultados de latencia para {title}._\n"
        rows = [
            "| Métrica | Valor |",
            "|---------|-------|",
        ]
        for k, v in stats.items():
            rows.append(f"| {k} | {fmt_float(v)} |")
        return f"\n**{title}**\n\n" + "\n".join(rows) + "\n"

    md.append(latency_table_section("Latencia en CPU (sklearn)", latency_cpu))
    md.append(latency_table_section("Latencia en ONNX Runtime", latency_onnx))
    md.append(latency_table_section("Latencia en TensorRT (GPU)", latency_trt))

    md.append("\n**Conclusiones de latencia:**\n")
    md.append(
        "- ONNX suele reducir la latencia respecto al modelo sklearn directo en CPU.\n"
    )
    md.append(
        "- TensorRT puede ofrecer latencias aún menores cuando se dispone de GPU, "
        "permitiendo cumplir con SLOs más exigentes.\n"
    )
    md.append(
        "- Los valores p95/p99 son clave para garantizar estabilidad en producción.\n"
    )

    # ===============================
    # 4. Percepción de usuarios
    # ===============================
    md.append(section_title("Percepción de usuarios sobre el modelo"))

    if perception:
        md.append("Los usuarios evaluaron el sistema en base a criterios de tipo Likert (1–5):\n\n")
        md.append("| Indicador | Promedio |\n")
        md.append("|-----------|----------|\n")

        for key, val in perception.items():
            if key.endswith("_promedio") or key == "score_global":
                md.append(f"| {key} | {fmt_float(val)} |\n")

        md.append("\n**Interpretación:**\n")
        md.append(
            "- Valores cercanos a 5 indican alta aceptación y confianza en el sistema.\n"
        )
        md.append(
            "- La métrica `score_global` resume la percepción general de utilidad, "
            "claridad y satisfacción.\n"
        )
    else:
        md.append(
            "No se encontraron métricas de percepción de usuarios en "
            "`perception_usuarios.csv`. Verificar la ejecución del script "
            "`08_evaluacion_percepcion.py`.\n"
        )

    # ===============================
    # 5. Pruebas de carga y estabilidad
    # ===============================
    md.append(section_title("Pruebas de carga y estabilidad (Locust)"))

    md.append(
        "Se realizaron pruebas de carga utilizando **Locust** contra el endpoint "
        "de inferencia `/predict`. Estas pruebas permiten observar el rendimiento "
        "bajo múltiples usuarios concurrentes.\n\n"
        "Los resultados detallados pueden consultarse en los reportes generados "
        "por Locust; en este informe se resume cualitativamente:\n\n"
        "- Verificación de estabilidad bajo carga moderada.\n"
        "- Observación de tiempos de respuesta promedio y p95.\n"
        "- Detección de posibles cuellos de botella en el backend.\n"
    )

    # ===============================
    # 6. Conclusiones y recomendación
    # ===============================
    md.append(section_title("Conclusiones y recomendación para despliegue"))

    md.append(
        "En base a la evidencia técnica y la percepción de usuarios, se concluye:\n\n"
    )
    md.append(
        "- El modelo actual muestra mejoras frente al baseline en las principales "
        "métricas de desempeño.\n"
    )
    md.append(
        "- La latencia obtenida en ONNX (y TensorRT, si aplica) resulta compatible "
        "con un escenario de uso operativo.\n"
    )
    md.append(
        "- La percepción de los usuarios indica que el sistema es útil, comprensible "
        "y genera un nivel de confianza aceptable.\n\n"
    )

    md.append(
        "**Recomendación:** avanzar hacia un despliegue controlado (por ejemplo, "
        "en modo *shadow* o *canary*), acompañado de monitoreo continuo de "
        "métricas de desempeño y recalibración periódica del modelo.\n"
    )

    return "".join(md)


# ============================================================
# MAIN
# ============================================================

def main(args: argparse.Namespace) -> None:
    logger = setup_logger(args.log_path)
    logger.info("=== Generando informe Sprint 4 ===")

    # Paths
    metrics_baseline_path = Path(args.metrics_baseline)
    metrics_model_path = Path(args.metrics_model)
    latency_cpu_path = Path(args.latency_cpu)
    latency_onnx_path = Path(args.latency_onnx)
    latency_trt_path = Path(args.latency_tensorrt)
    perception_path = Path(args.perception_csv)

    # Leer CSVs (muchos pueden no existir aún; se maneja gracefully)
    df_metrics_baseline = safe_read_csv(metrics_baseline_path)
    df_metrics_model = safe_read_csv(metrics_model_path)
    df_latency_cpu = safe_read_csv(latency_cpu_path)
    df_latency_onnx = safe_read_csv(latency_onnx_path)
    df_latency_trt = safe_read_csv(latency_trt_path)
    df_perception = safe_read_csv(perception_path)

    metrics_baseline = df_first_row_as_dict(df_metrics_baseline)
    metrics_model = df_first_row_as_dict(df_metrics_model)
    latency_cpu = df_first_row_as_dict(df_latency_cpu)
    latency_onnx = df_first_row_as_dict(df_latency_onnx)
    latency_trt = df_first_row_as_dict(df_latency_trt)
    perception = df_first_row_as_dict(df_perception)

    logger.info("Datos cargados. Construyendo markdown del informe…")

    report_md = build_report_markdown(
        metrics_baseline=metrics_baseline,
        metrics_model=metrics_model,
        latency_cpu=latency_cpu,
        latency_onnx=latency_onnx,
        latency_trt=latency_trt,
        perception=perception,
    )

    out_path = Path(args.out_path)
    ensure_parent_dir(out_path)

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report_md)

    logger.info(f"Informe Sprint 4 generado en: {out_path}")
    logger.info("=== Proceso completado ===")


# ============================================================
# CLI
# ============================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Genera el informe técnico consolidado del Sprint 4."
    )
    parser.add_argument(
        "--metrics-baseline",
        default="models/sprint4/resultados/metrics_baseline.csv",
        help="CSV con métricas del modelo baseline.",
    )
    parser.add_argument(
        "--metrics-model",
        default="models/sprint4/resultados/metrics_modelo_actual.csv",
        help="CSV con métricas del modelo actual.",
    )
    parser.add_argument(
        "--latency-cpu",
        default="models/sprint4/resultados/latencia_cpu.csv",
        help="CSV con resultados de latencia en CPU.",
    )
    parser.add_argument(
        "--latency-onnx",
        default="models/sprint4/resultados/latencia_onnx.csv",
        help="CSV con resultados de latencia en ONNX.",
    )
    parser.add_argument(
        "--latency-tensorrt",
        default="models/sprint4/resultados/latencia_tensorrt.csv",
        help="CSV con resultados de latencia TensorRT (opcional).",
    )
    parser.add_argument(
        "--perception-csv",
        default="models/sprint4/resultados/perception_usuarios.csv",
        help="CSV con métricas de percepción de usuarios.",
    )
    parser.add_argument(
        "--out-path",
        default="models/sprint4/reportes/informe_sprint4.md",
        help="Ruta donde se generará el informe en markdown.",
    )
    parser.add_argument(
        "--log-path",
        default="models/sprint4/logs/informe_sprint4.log",
        help="Archivo de log.",
    )

    main(parser.parse_args())

