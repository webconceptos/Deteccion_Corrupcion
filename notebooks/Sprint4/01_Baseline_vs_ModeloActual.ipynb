{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e97ddce9",
   "metadata": {},
   "source": [
    "### Sprint 4 – Notebook 01: Comparación Baseline vs Modelo Actual\n",
    "\n",
    "Este notebook analiza el desempeño del modelo baseline y del modelo actual optimizado.\n",
    "\n",
    "Incluye:\n",
    "\n",
    "- Carga de métricas generadas por los scripts del Sprint 4  \n",
    "- Comparación tabular  \n",
    "- Gráfico comparativo de métricas  \n",
    "- Ordenamiento por delta  \n",
    "- Comentarios y conclusiones  \n",
    "\n",
    "Las rutas ya están ajustadas para ejecutarse desde:  \n",
    "`notebooks/Sprint4/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48879fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE:\n",
    "# El notebook está en notebooks/Sprint4/\n",
    "# Por eso las rutas suben dos niveles (../../)\n",
    "\n",
    "metrics_baseline_path = Path(\"../../models/sprint4/resultados/metrics_baseline.csv\")\n",
    "metrics_model_path = Path(\"../../models/sprint4/resultados/metrics_modelo_actual.csv\")\n",
    "\n",
    "metrics_baseline_path, metrics_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c65a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not metrics_baseline_path.exists():\n",
    "    raise FileNotFoundError(f\"No existe {metrics_baseline_path}. Ejecuta 01_run_inference_baseline.py\")\n",
    "\n",
    "if not metrics_model_path.exists():\n",
    "    raise FileNotFoundError(f\"No existe {metrics_model_path}. Ejecuta 02_run_inference_model_actual.py\")\n",
    "\n",
    "mb = pd.read_csv(metrics_baseline_path)\n",
    "mm = pd.read_csv(metrics_model_path)\n",
    "\n",
    "print(\"Métricas baseline:\")\n",
    "display(mb)\n",
    "\n",
    "print(\"Métricas modelo actual:\")\n",
    "display(mm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_dict = mb.iloc[0].to_dict()\n",
    "mm_dict = mm.iloc[0].to_dict()\n",
    "\n",
    "all_keys = sorted(set(mb_dict.keys()) | set(mm_dict.keys()))\n",
    "\n",
    "rows = []\n",
    "for k in all_keys:\n",
    "    v_b = mb_dict.get(k, float(\"nan\"))\n",
    "    v_m = mm_dict.get(k, float(\"nan\"))\n",
    "    try:\n",
    "        delta = float(v_m) - float(v_b)\n",
    "    except:\n",
    "        delta = float(\"nan\")\n",
    "\n",
    "    rows.append({\n",
    "        \"metrica\": k,\n",
    "        \"baseline\": v_b,\n",
    "        \"modelo_actual\": v_m,\n",
    "        \"delta\": delta,\n",
    "    })\n",
    "\n",
    "df_comp = pd.DataFrame(rows)\n",
    "df_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_interes = [\n",
    "    \"accuracy\",\n",
    "    \"precision_macro\",\n",
    "    \"recall_macro\",\n",
    "    \"f1_macro\",\n",
    "    \"roc_auc\"\n",
    "]\n",
    "\n",
    "df_plot = df_comp[df_comp[\"metrica\"].isin(metricas_interes)].copy()\n",
    "df_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c45aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_plot.empty:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    x = range(len(df_plot))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(\n",
    "        [i - width/2 for i in x],\n",
    "        df_plot[\"baseline\"],\n",
    "        width=width,\n",
    "        label=\"Baseline\"\n",
    "    )\n",
    "    plt.bar(\n",
    "        [i + width/2 for i in x],\n",
    "        df_plot[\"modelo_actual\"],\n",
    "        width=width,\n",
    "        label=\"Modelo Actual\"\n",
    "    )\n",
    "\n",
    "    plt.xticks(list(x), df_plot[\"metrica\"], rotation=45)\n",
    "    plt.ylabel(\"Valor de la métrica\")\n",
    "    plt.title(\"Comparación de métricas – Baseline vs Modelo Actual\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay métricas para graficar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp_sorted = df_comp.sort_values(\"delta\", ascending=False)\n",
    "df_comp_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e10b4",
   "metadata": {},
   "source": [
    "#### Conclusiones preliminares\n",
    "\n",
    "- `f1_macro` y `recall_macro` suelen ser las métricas más relevantes cuando existe desbalance.  \n",
    "- `roc_auc` resume de forma global la capacidad discriminativa si está disponible.  \n",
    "- La columna `delta` permite cuantificar claramente la mejora del modelo actual frente al baseline.  \n",
    "\n",
    "Esta información se integra automáticamente en el informe generado por:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
