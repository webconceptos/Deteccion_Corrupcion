{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 · Construir Dataset Maestro (Unidad = Obra)\n",
    "\n",
    "Unifica **Obra + Empresa + Miembro** y usa **Matriz de Priorización** para etiquetar:\n",
    "- `PROYECTO_RIESGO` (numérico)\n",
    "- `PROYECTO_RIESGO_DESC` (categórico)\n",
    "\n",
    "Genera `data/processed/dataset_obras.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('c:/MaestriaUNI/Cursos/III-CICLO/TesisI/Solucion/Deteccion_Corrupcion'),\n",
       " WindowsPath('c:/MaestriaUNI/Cursos/III-CICLO/TesisI/Solucion/Deteccion_Corrupcion/data/external'),\n",
       " WindowsPath('c:/MaestriaUNI/Cursos/III-CICLO/TesisI/Solucion/Deteccion_Corrupcion/data/processed'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Detecta BASE como dos niveles arriba del directorio actual (asumiendo /notebooks/)\n",
    "BASE = Path.cwd().parents[0]\n",
    "DATA_EXT = BASE / 'data' / 'external'\n",
    "DATA_PROC = BASE / 'data' / 'processed'\n",
    "DATA_PROC.mkdir(parents=True, exist_ok=True)\n",
    "BASE, DATA_EXT, DATA_PROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Configuración (ajusta si cambian nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "  'obra_glob':        str(DATA_EXT / 'obra' / 'DS_DASH_Obra_*.csv'),\n",
    "  'empresa_glob':     str(DATA_EXT / 'empresa' / 'DS_DASH_Empresa_*.csv'),\n",
    "  'miembro_glob':     str(DATA_EXT / 'funcionario' / 'DS_DASH_Miembro_*.csv'),\n",
    "  'prio_glob':        str(DATA_EXT / 'priorizacion' / 'DS_DASH_MatrizPriorizacion_*.csv'),\n",
    "  # Claves oficiales provistas por el usuario\n",
    "  'key_obra':   'codigo_unico',\n",
    "  'key_ruc':    'codigo_ruc',\n",
    "  'key_dni':    'codigo_dni',\n",
    "  'key_prio':   'codigo_unico',  \n",
    "  # Targets en Matriz Priorización\n",
    "  'prio_num':   'PROYECTO_RIESGO',\n",
    "  'prio_desc':  'PROYECTO_RIESGO_DESC',\n",
    "  # Salida\n",
    "  'output_parquet': str(DATA_PROC / 'dataset_obras.parquet')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Utilitarios de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_any(path):\n",
    "    last_err = None\n",
    "    for sep in [',',';','\\t','|']:\n",
    "        for enc in ['utf-8','latin-1','cp1252']:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=sep, encoding=enc), enc, sep\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "    raise RuntimeError(f'No se pudo leer {path}: {last_err}')\n",
    "\n",
    "def cargar_por_glob(patron: str) -> pd.DataFrame:\n",
    "    paths = glob.glob(patron)\n",
    "    tablas = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df, enc, sep = read_csv_any(p)\n",
    "            tablas.append(df)\n",
    "            print(f'Cargado: {Path(p).name} -> {df.shape} (enc={enc}, sep={sep})')\n",
    "        except Exception as e:\n",
    "            print('ERROR leyendo', p, e)\n",
    "    return pd.concat(tablas, axis=0, ignore_index=True) if tablas else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Cargar datasets por entidad (concat por patrón)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patrón: c:\\MaestriaUNI\\Cursos\\III-CICLO\\TesisI\\Solucion\\Deteccion_Corrupcion\\data\\external\\obra\\DS_DASH_Obra_*.csv -> 8 archivos\n",
      "Cargado: DS_DASH_Obra_1A.csv -> (326, 12) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_2A_3A.csv -> (552, 22) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_2B.csv -> (1613, 6) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_3B.csv -> (5249, 20) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_3C.csv -> (5249, 8) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_4A.csv -> (326, 9) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_4B.csv -> (326, 15) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Obra_5A.csv -> (634, 15) (enc=latin-1, sep=,)\n",
      "Patrón: c:\\MaestriaUNI\\Cursos\\III-CICLO\\TesisI\\Solucion\\Deteccion_Corrupcion\\data\\external\\empresa\\DS_DASH_Empresa_*.csv -> 5 archivos\n",
      "Cargado: DS_DASH_Empresa_1A.csv -> (553, 9) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Empresa_1B.csv -> (2148, 5) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Empresa_2A.csv -> (371, 11) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Empresa_2B.csv -> (5395, 4) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Empresa_2C.csv -> (26, 2) (enc=utf-8, sep=,)\n",
      "Patrón: c:\\MaestriaUNI\\Cursos\\III-CICLO\\TesisI\\Solucion\\Deteccion_Corrupcion\\data\\external\\funcionario\\DS_DASH_Miembro_*.csv -> 4 archivos\n",
      "Cargado: DS_DASH_Miembro_1A.csv -> (1613, 10) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Miembro_2A.csv -> (739, 8) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Miembro_3A.csv -> (2606, 4) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_Miembro_3B.csv -> (643, 4) (enc=latin-1, sep=,)\n",
      "Patrón: c:\\MaestriaUNI\\Cursos\\III-CICLO\\TesisI\\Solucion\\Deteccion_Corrupcion\\data\\external\\priorizacion\\DS_DASH_MatrizPriorizacion_*.csv -> 3 archivos\n",
      "Cargado: DS_DASH_MatrizPriorizacion_1A.csv -> (822, 18) (enc=latin-1, sep=,)\n",
      "Cargado: DS_DASH_MatrizPriorizacion_2A.csv -> (2391, 5) (enc=utf-8, sep=,)\n",
      "Cargado: DS_DASH_MatrizPriorizacion_3A.csv -> (326, 6) (enc=latin-1, sep=,)\n",
      "Shapes (raw) -> Obra (14275, 55) | Empresa (8493, 25) | Miembro (5601, 16) | Matriz (3539, 26)\n",
      "OBRA: 171 claves únicas, 55 columnas.\n",
      "EMPRESA: 372 claves únicas, 25 columnas.\n",
      "MIEMBRO: 740 claves únicas, 16 columnas.\n",
      "MATRIZ: 822 claves únicas, 26 columnas.\n",
      "Shapes (unificados) -> Obra (171, 55) | Empresa (372, 25) | Miembro (740, 16) | Matriz (822, 26)\n"
     ]
    }
   ],
   "source": [
    "# === 3) Cargar datasets por entidad (concat por patrón) — VERSIÓN UNIFICADA ===\n",
    "import re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_csv_any(path):\n",
    "    last_err = None\n",
    "    for sep in [',',';','\\t','|']:\n",
    "        for enc in ['utf-8','latin-1','cp1252']:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, encoding=enc)\n",
    "                return df, enc, sep\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"No se pudo leer {path}: {last_err}\")\n",
    "\n",
    "def cargar_por_glob(patron: str) -> pd.DataFrame:\n",
    "    files = glob.glob(patron)\n",
    "    print(f\"Patrón: {patron} -> {len(files)} archivos\")\n",
    "    tablas = []\n",
    "    for p in files:\n",
    "        try:\n",
    "            df, enc, sep = read_csv_any(p)\n",
    "            tablas.append(df)\n",
    "            print(f\"Cargado: {Path(p).name} -> {df.shape} (enc={enc}, sep={sep})\")\n",
    "        except Exception as e:\n",
    "            print(\"ERROR leyendo\", p, e)\n",
    "    return pd.concat(tablas, axis=0, ignore_index=True) if tablas else pd.DataFrame()\n",
    "\n",
    "def norm_key_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normaliza valores de llave: string, sin .0 final, mayúscula, sin tildes ni separadores.\"\"\"\n",
    "    s = s.astype(str).str.strip().str.upper()\n",
    "    s = s.str.replace(r'\\.0$', '', regex=True)\n",
    "    s = s.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
    "    s = s.str.replace(r'[\\s\\-\\._/]', '', regex=True)\n",
    "    return s\n",
    "\n",
    "def ensure_col(df, target: str, candidates: list):\n",
    "    \"\"\"Renombra la primera columna existente entre candidates -> target (case/guiones/espacios tolerantes).\"\"\"\n",
    "    def _norm(x): return re.sub(r'[\\s_\\-]+','', str(x).lower())\n",
    "    if target in df.columns:\n",
    "        return df\n",
    "    norm_map = {_norm(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        nc = _norm(cand)\n",
    "        if nc in norm_map:\n",
    "            return df.rename(columns={norm_map[nc]: target})\n",
    "    # heurística adicional para codigo_unico / ruc / dni\n",
    "    if target == 'codigo_unico':\n",
    "        for c in df.columns:\n",
    "            nc = _norm(c)\n",
    "            if 'cod' in nc and 'unico' in nc:\n",
    "                return df.rename(columns={c: target})\n",
    "    if target == 'codigo_ruc':\n",
    "        for c in df.columns:\n",
    "            if 'ruc' in c.lower(): return df.rename(columns={c: target})\n",
    "    if target == 'codigo_dni':\n",
    "        for c in df.columns:\n",
    "            if 'dni' in c.lower() or 'document' in c.lower(): return df.rename(columns={c: target})\n",
    "    return df\n",
    "\n",
    "def unify_group(df: pd.DataFrame, key: str, key_cands: list, prefer_cols: list = None, name: str = '') -> pd.DataFrame:\n",
    "    \"\"\"Estandariza clave, normaliza valores y deja una sola fila por clave (más completa).\"\"\"\n",
    "    if df.empty:\n",
    "        print(f\"{name}: vacío.\")\n",
    "        return df\n",
    "    df = ensure_col(df, key, key_cands)\n",
    "    if key not in df.columns:\n",
    "        raise KeyError(f\"{name}: no se encontró columna clave '{key}'. Columnas: {list(df.columns)[:30]}\")\n",
    "    # normalizar valores de llave\n",
    "    df[key] = norm_key_series(df[key])\n",
    "    # score de completitud (prioriza columnas preferidas si existen)\n",
    "    if prefer_cols:\n",
    "        use = [c for c in prefer_cols if c in df.columns]\n",
    "        if not use:\n",
    "            use = [c for c in df.columns if c != key]\n",
    "    else:\n",
    "        use = [c for c in df.columns if c != key]\n",
    "    df['_non_na_score'] = df[use].notna().sum(axis=1)\n",
    "    # quedarnos con la fila más completa por clave\n",
    "    df = (df.sort_values([key, '_non_na_score'], ascending=[True, False])\n",
    "            .drop_duplicates(subset=[key], keep='first')\n",
    "            .drop(columns=['_non_na_score']))\n",
    "    print(f\"{name}: {df.shape[0]} claves únicas, {df.shape[1]} columnas.\")\n",
    "    return df\n",
    "\n",
    "# 1) Cargar TODOS los CSV por grupo\n",
    "df_obra_raw = cargar_por_glob(CONFIG['obra_glob'])\n",
    "df_emp_raw  = cargar_por_glob(CONFIG['empresa_glob'])\n",
    "df_mbr_raw  = cargar_por_glob(CONFIG['miembro_glob'])\n",
    "df_prio_raw = cargar_por_glob(CONFIG['prio_glob'])\n",
    "\n",
    "print(\"Shapes (raw) ->\",\n",
    "      \"Obra\", df_obra_raw.shape, \"| Empresa\", df_emp_raw.shape,\n",
    "      \"| Miembro\", df_mbr_raw.shape, \"| Matriz\", df_prio_raw.shape)\n",
    "\n",
    "# 2) Unificar por clave en cada grupo (una fila por clave)\n",
    "df_obra = unify_group(\n",
    "    df_obra_raw.copy(),\n",
    "    key=CONFIG['key_obra'],\n",
    "    key_cands=['CODIGO_UNICO','codigo_unico','CODIGO_OBRA','IDENTIFICADOR_OBRA','id_obra','id_seace','snip'],\n",
    "    # puedes priorizar columnas típicas de obra si quieres (monto/plazo/ubicación):\n",
    "    prefer_cols=[c for c in ['codigo_ruc','codigo_dni','monto','valor_referencial','plazo','departamento','provincia','distrito'] if c in df_obra_raw.columns],\n",
    "    name='OBRA'\n",
    ")\n",
    "\n",
    "df_emp = unify_group(\n",
    "    df_emp_raw.copy(),\n",
    "    key=CONFIG['key_ruc'],\n",
    "    key_cands=['codigo_ruc','RUC','ruc','ruc_empresa','ruc_proveedor','COD_RUC'],\n",
    "    prefer_cols=[c for c in df_emp_raw.columns if any(k in c.lower() for k in ['sancio','inhabil','arbitra','penal','multa','riesg'])],\n",
    "    name='EMPRESA'\n",
    ")\n",
    "\n",
    "df_mbr = unify_group(\n",
    "    df_mbr_raw.copy(),\n",
    "    key=CONFIG['key_dni'],\n",
    "    key_cands=['codigo_dni','DNI','dni','dni_funcionario','documento','doc_identidad','id_miembro'],\n",
    "    prefer_cols=[c for c in df_mbr_raw.columns if any(k in c.lower() for k in ['sancio','respons','observa','riesg'])],\n",
    "    name='MIEMBRO'\n",
    ")\n",
    "\n",
    "# Para la Matriz, priorizamos los targets\n",
    "df_prio = unify_group(\n",
    "    df_prio_raw.copy()\n",
    "       .rename(columns={'COD_UNICO':'CODIGO_UNICO'}),  # normalizamos nombre más común\n",
    "    key=CONFIG['key_obra'],  # debe quedar en 'codigo_unico'\n",
    "    key_cands=['CODIGO_UNICO','codigo_unico','CODIGO_OBRA','IDENTIFICADOR_OBRA','ID_OBRA'],\n",
    "    prefer_cols=[c for c in [CONFIG['prio_num'], CONFIG['prio_desc'], 'OBRA_RIESGO','OBRA_RIESGO_DESC','CODIGO_OBRA','IDENTIFICADOR_OBRA'] if c in df_prio_raw.columns],\n",
    "    name='MATRIZ'\n",
    ")\n",
    "\n",
    "print(\"Shapes (unificados) ->\",\n",
    "      \"Obra\", df_obra.shape, \"| Empresa\", df_emp.shape,\n",
    "      \"| Miembro\", df_mbr.shape, \"| Matriz\", df_prio.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Estandarizar claves (renombrar si llegan con variantes)\n",
    "Usamos las claves oficiales: `codigo_unico`, `codigo_ruc`, `codigo_dni`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBRA] clave='codigo_unico' | filas: 171 -> 171 (únicas) | ncols: 55\n",
      "[EMPRESA] clave='codigo_ruc' | filas: 372 -> 372 (únicas) | ncols: 25\n",
      "[MIEMBRO] clave='codigo_dni' | filas: 740 -> 740 (únicas) | ncols: 16\n",
      "[MATRIZ] clave='codigo_unico' | filas: 822 -> 822 (únicas) | ncols: 26\n",
      "Intersección Obra∩Matriz: 170 / 171\n",
      "Claves únicas -> Obra:171  Matriz:822  Empresa:372  Miembro:740\n"
     ]
    }
   ],
   "source": [
    "# ==== 4) Estandarizar claves (robusto) ====\n",
    "# Objetivo:\n",
    "# - Garantizar que las llaves queden con nombres estándar:\n",
    "#     Obra/Matriz -> codigo_unico\n",
    "#     Empresa     -> codigo_ruc\n",
    "#     Miembro     -> codigo_dni\n",
    "# - Normalizar los valores de llave (string, sin .0, sin separadores, mayúscula)\n",
    "# - Quitar duplicados por llave\n",
    "# - Dejar trazas de cobertura\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def _norm_name(s: str) -> str:\n",
    "    return re.sub(r'[\\s_\\-]+', '', str(s).lower())\n",
    "\n",
    "def ensure_col(df: pd.DataFrame, target: str, candidates: list) -> pd.DataFrame:\n",
    "    \"\"\"Renombra a 'target' la 1a columna que matchee (por nombre normalizado).\"\"\"\n",
    "    if target in df.columns:\n",
    "        return df\n",
    "    norm_map = {_norm_name(c): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        nc = _norm_name(cand)\n",
    "        if nc in norm_map:\n",
    "            return df.rename(columns={norm_map[nc]: target})\n",
    "    # heurística por tokens\n",
    "    for c in df.columns:\n",
    "        nc = _norm_name(c)\n",
    "        if target == 'codigo_unico' and ('cod' in nc and 'unico' in nc):\n",
    "            return df.rename(columns={c: target})\n",
    "        if target == 'codigo_ruc' and 'ruc' in nc:\n",
    "            return df.rename(columns={c: target})\n",
    "        if target == 'codigo_dni' and ('dni' in nc or 'document' in nc):\n",
    "            return df.rename(columns={c: target})\n",
    "    return df\n",
    "\n",
    "def norm_key_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip().str.upper()\n",
    "    s = s.str.replace(r'\\.0$', '', regex=True)  # quita .0 al final\n",
    "    s = s.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
    "    s = s.str.replace(r'[\\s\\-\\._/]', '', regex=True)\n",
    "    return s\n",
    "\n",
    "def standardize_key(df: pd.DataFrame, key: str, key_cands: list, name: str) -> pd.DataFrame:\n",
    "    df = ensure_col(df, key, key_cands)\n",
    "    if key not in df.columns:\n",
    "        raise KeyError(f\"[{name}] No se encontró la clave '{key}'. Columnas: {list(df.columns)[:30]}\")\n",
    "    # normalizar valores y deduplicar\n",
    "    df[key] = norm_key_series(df[key])\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=[key], keep='first')\n",
    "    print(f\"[{name}] clave='{key}' | filas: {before} -> {len(df)} (únicas) | ncols: {df.shape[1]}\")\n",
    "    return df\n",
    "\n",
    "# --- Aplicar a cada grupo ---\n",
    "df_obra = standardize_key(\n",
    "    df_obra, key='codigo_unico',\n",
    "    key_cands=['CODIGO_UNICO','COD_UNICO','CODIGO_OBRA','IDENTIFICADOR_OBRA','ID_OBRA','ID_SEACE','SNIP'],\n",
    "    name='OBRA'\n",
    ")\n",
    "\n",
    "df_emp = standardize_key(\n",
    "    df_emp, key='codigo_ruc',\n",
    "    key_cands=['codigo_ruc','RUC','ruc','ruc_empresa','ruc_proveedor','COD_RUC'],\n",
    "    name='EMPRESA'\n",
    ")\n",
    "\n",
    "df_mbr = standardize_key(\n",
    "    df_mbr, key='codigo_dni',\n",
    "    key_cands=['codigo_dni','DNI','dni','dni_funcionario','documento','doc_identidad','id_miembro'],\n",
    "    name='MIEMBRO'\n",
    ")\n",
    "\n",
    "# La matriz se alinea a la llave de OBRA\n",
    "df_prio = standardize_key(\n",
    "    df_prio.rename(columns={'COD_UNICO':'CODIGO_UNICO'}),  # por si vino así\n",
    "    key='codigo_unico',\n",
    "    key_cands=['CODIGO_UNICO','COD_UNICO','CODIGO_OBRA','IDENTIFICADOR_OBRA','ID_OBRA'],\n",
    "    name='MATRIZ'\n",
    ")\n",
    "\n",
    "# --- Diagnóstico rápido de intersección de llaves (previo al merge de la sección 7) ---\n",
    "obra_keys   = set(df_obra['codigo_unico'])\n",
    "prio_keys   = set(df_prio['codigo_unico']) if 'codigo_unico' in df_prio.columns else set()\n",
    "emp_keys    = set(df_emp['codigo_ruc'])    if 'codigo_ruc' in df_emp.columns else set()\n",
    "mbr_keys    = set(df_mbr['codigo_dni'])    if 'codigo_dni' in df_mbr.columns else set()\n",
    "\n",
    "print(f\"Intersección Obra∩Matriz: {len(obra_keys & prio_keys)} / {len(obra_keys)}\")\n",
    "print(f\"Claves únicas -> Obra:{len(obra_keys)}  Matriz:{len(prio_keys)}  Empresa:{len(emp_keys)}  Miembro:{len(mbr_keys)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Agregados por Empresa (RUC) y Miembro (DNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo_ruc</th>\n",
       "      <th>SANCIONADAS_TCE_FLAG</th>\n",
       "      <th>INHABILITADAS_PJ_FLAG</th>\n",
       "      <th>SANCIONADAS_RNP_FLAG</th>\n",
       "      <th>INHABILITADAS_RNP_FLAG</th>\n",
       "      <th>NUMERO_SANCIONES_TCE</th>\n",
       "      <th>NUMERO_SANCIONES_RNP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>10001046191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>10011010534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>10011217104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>10011600447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>10020343112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>10021511116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>10024475005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>10031271369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      codigo_ruc  SANCIONADAS_TCE_FLAG  INHABILITADAS_PJ_FLAG  \\\n",
       "172  10001046191                   0.0                    0.0   \n",
       "381  10011010534                   0.0                    0.0   \n",
       "274  10011217104                   0.0                    0.0   \n",
       "267  10011600447                   0.0                    0.0   \n",
       "182  10020343112                   0.0                    0.0   \n",
       "463  10021511116                   0.0                    0.0   \n",
       "427  10024475005                   0.0                    0.0   \n",
       "139  10031271369                   0.0                    0.0   \n",
       "\n",
       "     SANCIONADAS_RNP_FLAG  INHABILITADAS_RNP_FLAG  NUMERO_SANCIONES_TCE  \\\n",
       "172                   0.0                     0.0                   0.0   \n",
       "381                   0.0                     0.0                   0.0   \n",
       "274                   0.0                     0.0                   0.0   \n",
       "267                   0.0                     0.0                   0.0   \n",
       "182                   0.0                     0.0                   0.0   \n",
       "463                   0.0                     0.0                   0.0   \n",
       "427                   0.0                     0.0                   0.0   \n",
       "139                   0.0                     0.0                   0.0   \n",
       "\n",
       "     NUMERO_SANCIONES_RNP  \n",
       "172                   0.0  \n",
       "381                   0.0  \n",
       "274                   0.0  \n",
       "267                   0.0  \n",
       "182                   0.0  \n",
       "463                   0.0  \n",
       "427                   0.0  \n",
       "139                   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empresa agregados: (372, 20) | cols: 19\n",
      "Miembro agregados: (740, 11) | cols: 10\n"
     ]
    }
   ],
   "source": [
    "# ==== 5) Agregados por Empresa (RUC) y Miembro (DNI) ====\n",
    "# Requiere: df_emp (clave codigo_ruc) y df_mbr (clave codigo_dni) ya estandarizados (Secc. 3 y 4)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def to_numeric_safe(df, cols_like):\n",
    "    \"\"\"Conserva sólo columnas que matchean 'cols_like' y las convierte a numérico.\n",
    "       Mapea SI/NO/True/False → 1/0 cuando aplique.\"\"\"\n",
    "    cols = [c for c in df.columns if any(k in c.lower() for k in cols_like)]\n",
    "    if not cols:\n",
    "        return pd.DataFrame(index=df.index)\n",
    "    tmp = df[cols].copy()\n",
    "\n",
    "    for c in tmp.columns:\n",
    "        if tmp[c].dtype == object:\n",
    "            s = tmp[c].astype(str).str.strip().str.lower()\n",
    "            mask = s.isin(['si','sí','true','t','1','no','false','f','0'])\n",
    "            if mask.any():\n",
    "                s = s.replace({'si':1,'sí':1,'true':1,'t':1,'1':1,\n",
    "                               'no':0,'false':0,'f':0,'0':0})\n",
    "                tmp[c] = pd.to_numeric(s, errors='coerce')\n",
    "            else:\n",
    "                tmp[c] = pd.to_numeric(tmp[c], errors='coerce')\n",
    "        else:\n",
    "            tmp[c] = pd.to_numeric(tmp[c], errors='coerce')\n",
    "\n",
    "    keep = [c for c in tmp.columns if tmp[c].notna().any()]\n",
    "    return tmp[keep]\n",
    "\n",
    "\n",
    "def aggregate_by_key(df, key, risk_like, prefix):\n",
    "    \"\"\"sum/mean/max por clave en columnas numéricas detectadas por 'risk_like'.\"\"\"\n",
    "    if key not in df.columns or df.empty:\n",
    "        return pd.DataFrame()\n",
    "    numdf = to_numeric_safe(df, risk_like)\n",
    "    if numdf.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    agg = (\n",
    "        pd.concat([df[[key]], numdf], axis=1)\n",
    "          .groupby(key)\n",
    "          .agg(['sum','mean','max'])\n",
    "    )\n",
    "    agg.columns = [f\"{prefix}__{col}_{stat}\" for col, stat in agg.columns]\n",
    "    agg = agg.reset_index()\n",
    "\n",
    "    # Añadir conteo de registros por clave\n",
    "    counts = df.groupby(key).size().rename(f\"{prefix}__n_registros\").reset_index()\n",
    "    agg = agg.merge(counts, on=key, how='left')\n",
    "    return agg\n",
    "\n",
    "# --- Normalización previa para Empresa: estados -> 0/1 y conteos numéricos\n",
    "import re\n",
    "def estado_to_flag(s):\n",
    "    if pd.isna(s): \n",
    "        return np.nan\n",
    "    x = str(s).strip().upper()\n",
    "    if 'NORMAL' in x:\n",
    "        return 0\n",
    "    # cualquier señal de sanción/inhabilitación/suspensión/impedimento/multa\n",
    "    if re.search(r'(SANCION|INHABIL|SUSPEN|IMPED|MULTA)', x):\n",
    "        return 1\n",
    "    return np.nan  # si no matchea nada, lo dejamos como NaN\n",
    "\n",
    "# columnas categóricas de estado (ajusta si ves otras)\n",
    "estado_cols = [\n",
    "    'SANCIONADAS_TCE','INHABILITADAS_PJ','SANCIONADAS_RNP','INHABILITADAS_RNP'\n",
    "]\n",
    "estado_cols = [c for c in estado_cols if c in df_emp.columns]\n",
    "\n",
    "for c in estado_cols:\n",
    "    df_emp[c + '_FLAG'] = df_emp[c].apply(estado_to_flag)\n",
    "\n",
    "# columnas de conteo (asegurar numérico)\n",
    "count_cols = ['NUMERO_SANCIONES_TCE','NUMERO_SANCIONES_RNP']\n",
    "for c in [cc for cc in count_cols if c in df_emp.columns]:\n",
    "    df_emp[c] = pd.to_numeric(df_emp[c], errors='coerce').fillna(0)\n",
    "\n",
    "# (opcional) muestra rápida\n",
    "cols_show = ['codigo_ruc'] + [c+'_FLAG' for c in estado_cols] + [c for c in count_cols if c in df_emp.columns]\n",
    "display(df_emp[cols_show].head(8))\n",
    "\n",
    "\n",
    "# --- Empresa (RUC) ---\n",
    "EMP_LIKE = ['riesg','sancio','inhabil','arbitra','penal','multa','mora']\n",
    "df_emp_agg = aggregate_by_key(df_emp, 'codigo_ruc', EMP_LIKE, prefix='empresa')\n",
    "print('Empresa agregados:', df_emp_agg.shape, '| cols:', len([c for c in df_emp_agg.columns if c.startswith('empresa__')]))\n",
    "\n",
    "# --- Miembro (DNI) ---\n",
    "MBR_LIKE = ['riesg','sancio','respons','observa','inhabil']\n",
    "df_mbr_agg = aggregate_by_key(df_mbr, 'codigo_dni', MBR_LIKE, prefix='miembro')\n",
    "print('Miembro agregados:', df_mbr_agg.shape, '| cols:', len([c for c in df_mbr_agg.columns if c.startswith('miembro__')]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiene clave codigo_ruc en df_emp?: True\n",
      "Filas no nulas en codigo_ruc: 372\n",
      "Columnas empresa que matchean patrones: ['SANCIONADAS_TCE', 'INHABILITADAS_PJ', 'SANCIONADAS_RNP', 'INHABILITADAS_RNP', 'NUMERO_SANCIONES_TCE', 'NUMERO_SANCIONES_RNP', 'SANCIONADAS_TCE_FLAG', 'INHABILITADAS_PJ_FLAG', 'SANCIONADAS_RNP_FLAG', 'INHABILITADAS_RNP_FLAG']  (total: 10 )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo_ruc</th>\n",
       "      <th>SANCIONADAS_TCE</th>\n",
       "      <th>INHABILITADAS_PJ</th>\n",
       "      <th>SANCIONADAS_RNP</th>\n",
       "      <th>INHABILITADAS_RNP</th>\n",
       "      <th>NUMERO_SANCIONES_TCE</th>\n",
       "      <th>NUMERO_SANCIONES_RNP</th>\n",
       "      <th>SANCIONADAS_TCE_FLAG</th>\n",
       "      <th>INHABILITADAS_PJ_FLAG</th>\n",
       "      <th>SANCIONADAS_RNP_FLAG</th>\n",
       "      <th>INHABILITADAS_RNP_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>10001046191</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>10011010534</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>10011217104</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>10011600447</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>10020343112</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>EN SITUACION NORMAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      codigo_ruc      SANCIONADAS_TCE     INHABILITADAS_PJ  \\\n",
       "172  10001046191  EN SITUACION NORMAL  EN SITUACION NORMAL   \n",
       "381  10011010534  EN SITUACION NORMAL  EN SITUACION NORMAL   \n",
       "274  10011217104  EN SITUACION NORMAL  EN SITUACION NORMAL   \n",
       "267  10011600447  EN SITUACION NORMAL  EN SITUACION NORMAL   \n",
       "182  10020343112  EN SITUACION NORMAL  EN SITUACION NORMAL   \n",
       "\n",
       "         SANCIONADAS_RNP    INHABILITADAS_RNP  NUMERO_SANCIONES_TCE  \\\n",
       "172  EN SITUACION NORMAL  EN SITUACION NORMAL                   0.0   \n",
       "381  EN SITUACION NORMAL  EN SITUACION NORMAL                   0.0   \n",
       "274  EN SITUACION NORMAL  EN SITUACION NORMAL                   0.0   \n",
       "267  EN SITUACION NORMAL  EN SITUACION NORMAL                   0.0   \n",
       "182  EN SITUACION NORMAL  EN SITUACION NORMAL                   0.0   \n",
       "\n",
       "     NUMERO_SANCIONES_RNP  SANCIONADAS_TCE_FLAG  INHABILITADAS_PJ_FLAG  \\\n",
       "172                   0.0                   0.0                    0.0   \n",
       "381                   0.0                   0.0                    0.0   \n",
       "274                   0.0                   0.0                    0.0   \n",
       "267                   0.0                   0.0                    0.0   \n",
       "182                   0.0                   0.0                    0.0   \n",
       "\n",
       "     SANCIONADAS_RNP_FLAG  INHABILITADAS_RNP_FLAG  \n",
       "172                   0.0                     0.0  \n",
       "381                   0.0                     0.0  \n",
       "274                   0.0                     0.0  \n",
       "267                   0.0                     0.0  \n",
       "182                   0.0                     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Tiene clave codigo_ruc en df_emp?:\", 'codigo_ruc' in df_emp.columns)\n",
    "print(\"Filas no nulas en codigo_ruc:\", df_emp['codigo_ruc'].notna().sum() if 'codigo_ruc' in df_emp.columns else 0)\n",
    "\n",
    "EMP_LIKE = ['riesg','sancio','inhabil','arbitra','penal','multa','mora']\n",
    "emp_match_cols = [c for c in df_emp.columns if any(k in c.lower() for k in EMP_LIKE)]\n",
    "print(\"Columnas empresa que matchean patrones:\", emp_match_cols[:30], \" (total:\", len(emp_match_cols), \")\")\n",
    "\n",
    "# Mira 5 filas de esas columnas\n",
    "if emp_match_cols:\n",
    "    display(df_emp[['codigo_ruc'] + emp_match_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Merge: enriquecer Obra con agregados por RUC y DNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 55)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== 6) Merge: enriquecer Obra con agregados por RUC y DNI (robusto) ====\n",
    "df_maestro = df_obra.copy()\n",
    "\n",
    "# Detectar columnas RUC/DNI en Obra y estandarizarlas localmente\n",
    "RUC_OBRA_CANDS = ['codigo_ruc','ruc','ruc_empresa','ruc_proveedor']\n",
    "DNI_OBRA_CANDS = ['codigo_dni','dni','dni_funcionario','documento','doc_identidad','id_miembro']\n",
    "\n",
    "def pick_col(df, cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "col_ruc_obra = pick_col(df_maestro, RUC_OBRA_CANDS)\n",
    "col_dni_obra = pick_col(df_maestro, DNI_OBRA_CANDS)\n",
    "\n",
    "if col_ruc_obra:\n",
    "    df_maestro.rename(columns={col_ruc_obra: 'codigo_ruc_obra'}, inplace=True)\n",
    "if col_dni_obra:\n",
    "    df_maestro.rename(columns={col_dni_obra: 'codigo_dni_obra'}, inplace=True)\n",
    "\n",
    "def norm_key(s):\n",
    "    return (s.astype(str).str.strip().str.upper()\n",
    "              .str.replace(r'\\.0$','',regex=True)\n",
    "              .str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
    "              .str.replace(r'[\\s\\-\\._/]','',regex=True))\n",
    "\n",
    "# Normalizar llaves en todos\n",
    "if 'codigo_ruc_obra' in df_maestro.columns:\n",
    "    df_maestro['codigo_ruc_obra'] = norm_key(df_maestro['codigo_ruc_obra'])\n",
    "if 'codigo_dni_obra' in df_maestro.columns:\n",
    "    df_maestro['codigo_dni_obra'] = norm_key(df_maestro['codigo_dni_obra'])\n",
    "\n",
    "if not df_emp_agg.empty:\n",
    "    df_emp_agg['codigo_ruc'] = norm_key(df_emp_agg['codigo_ruc'])\n",
    "    df_emp_agg = df_emp_agg.drop_duplicates('codigo_ruc')\n",
    "if not df_mbr_agg.empty:\n",
    "    df_mbr_agg['codigo_dni'] = norm_key(df_mbr_agg['codigo_dni'])\n",
    "    df_mbr_agg = df_mbr_agg.drop_duplicates('codigo_dni')\n",
    "\n",
    "# Merge + cobertura\n",
    "if not df_emp_agg.empty and 'codigo_ruc_obra' in df_maestro.columns:\n",
    "    before = len(df_maestro)\n",
    "    df_maestro = df_maestro.merge(df_emp_agg, left_on='codigo_ruc_obra', right_on='codigo_ruc', how='left')\n",
    "    matched_emp = df_maestro['codigo_ruc'].notna().sum()\n",
    "    print(f'Empresa: {matched_emp}/{before} obras con match por RUC')\n",
    "\n",
    "if not df_mbr_agg.empty and 'codigo_dni_obra' in df_maestro.columns:\n",
    "    before = len(df_maestro)\n",
    "    df_maestro = df_maestro.merge(df_mbr_agg, left_on='codigo_dni_obra', right_on='codigo_dni', how='left')\n",
    "    matched_mbr = df_maestro['codigo_dni'].notna().sum()\n",
    "    print(f'Miembro : {matched_mbr}/{before} obras con match por DNI')\n",
    "\n",
    "df_maestro.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Etiquetado desde **Matriz de Priorización** (y fallback por reglas)\n",
    "- Si existen `PROYECTO_RIESGO` y/o `PROYECTO_RIESGO_DESC` en la matriz, se mapean a `y_riesgo`.\n",
    "- Obras sin etiqueta en matriz se completan con **reglas** (penalidades/arbitrajes/adicionales/ampliaciones/rescisión/sanciones/observaciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersección de JOIN_KEY: 170\n",
      "Cobertura etiqueta en intersección:\n",
      " - num no nulos: {'PROYECTO_RIESGO': 0, 'OBRA_RIESGO': 170}\n",
      " - desc no nulos: {'PROYECTO_RIESGO_DESC': 0, 'OBRA_RIESGO_DESC': 170}\n",
      "Eliminando del izquierdo por colisión: ['PROYECTO_RIESGO', 'PROYECTO_RIESGO_DESC']\n",
      "Etiquetas numéricas traídas: 170 | descriptivas traídas: 170\n",
      "Distribución final y_riesgo:\n",
      "y_riesgo\n",
      "1    153\n",
      "0     18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==== MATRIZ: construir tabla de llaves + TODOS los candidatos de etiqueta ====\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "def norm_key(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip().str.upper()\n",
    "    s = s.str.replace(r'\\.0$','', regex=True)\n",
    "    s = s.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('ascii')\n",
    "    s = s.str.replace(r'[\\s\\-\\._/]', '', regex=True)\n",
    "    return s\n",
    "\n",
    "def coalesce_cols(df, cols):\n",
    "    out = pd.Series(pd.NA, index=df.index, dtype=\"object\")\n",
    "    for c in [c for c in cols if c in df.columns]:\n",
    "        out = out.where(out.notna(), df[c])\n",
    "    return out\n",
    "\n",
    "# 1) JOIN_KEY en obra y matriz (coalesce de IDs más comunes)\n",
    "obra_ids   = ['CODIGO_UNICO','codigo_unico','CODIGO_OBRA','IDENTIFICADOR_OBRA','ID_OBRA']\n",
    "matriz_ids = ['COD_UNICO','CODIGO_UNICO','codigo_unico','CODIGO_OBRA','IDENTIFICADOR_OBRA','ID_OBRA']\n",
    "\n",
    "df_maestro['JOIN_KEY'] = norm_key(coalesce_cols(df_maestro, obra_ids))\n",
    "df_prio['_JOIN_KEY']   = norm_key(coalesce_cols(df_prio, matriz_ids))\n",
    "\n",
    "# 2) Elegir columnas de etiqueta candidatas\n",
    "label_num_cols  = [c for c in ['PROYECTO_RIESGO','OBRA_RIESGO'] if c in df_prio.columns]\n",
    "label_desc_cols = [c for c in ['PROYECTO_RIESGO_DESC','OBRA_RIESGO_DESC'] if c in df_prio.columns]\n",
    "\n",
    "keep_cols = ['_JOIN_KEY'] + label_num_cols + label_desc_cols\n",
    "df_prio_keys = df_prio[keep_cols].dropna(subset=['_JOIN_KEY']).copy()\n",
    "\n",
    "# 3) Para cada JOIN_KEY, quedarnos con la fila que tenga más etiquetas no nulas\n",
    "score_cols = label_num_cols + label_desc_cols\n",
    "if score_cols:\n",
    "    df_prio_keys['_score'] = df_prio_keys[score_cols].notna().sum(axis=1)\n",
    "    df_prio_keys = (df_prio_keys\n",
    "                    .sort_values(['_JOIN_KEY','_score'], ascending=[True, False])\n",
    "                    .drop_duplicates('_JOIN_KEY', keep='first')\n",
    "                    .drop(columns=['_score']))\n",
    "\n",
    "df_prio_keys = df_prio_keys.rename(columns={'_JOIN_KEY':'JOIN_KEY'})\n",
    "\n",
    "# 4) Diagnóstico de cobertura antes del merge\n",
    "inter = set(df_maestro['JOIN_KEY'].dropna().unique()) & set(df_prio_keys['JOIN_KEY'].dropna().unique())\n",
    "print(\"Intersección de JOIN_KEY:\", len(inter))\n",
    "print(\"Cobertura etiqueta en intersección:\")\n",
    "if inter:\n",
    "    sub = df_prio_keys[df_prio_keys['JOIN_KEY'].isin(inter)]\n",
    "    print(\" - num no nulos:\", sub[label_num_cols].notna().sum().to_dict() if label_num_cols else {})\n",
    "    print(\" - desc no nulos:\", sub[label_desc_cols].notna().sum().to_dict() if label_desc_cols else {})\n",
    "\n",
    "# 5) Evitar colisiones con columnas ya presentes\n",
    "cols_right = set(df_prio_keys.columns) - {'JOIN_KEY'}\n",
    "cols_overlap = list(set(df_maestro.columns) & cols_right)\n",
    "if cols_overlap:\n",
    "    print(\"Eliminando del izquierdo por colisión:\", cols_overlap)\n",
    "    df_maestro = df_maestro.drop(columns=cols_overlap, errors='ignore')\n",
    "to_drop_prev = [c for c in df_maestro.columns if c.endswith('_x') or c.endswith('_y')]\n",
    "if to_drop_prev:\n",
    "    print(\"Limpiando restos de merges previos:\", to_drop_prev)\n",
    "    df_maestro = df_maestro.drop(columns=to_drop_prev, errors='ignore')\n",
    "\n",
    "# 6) MERGE (m:1)\n",
    "before = len(df_maestro)\n",
    "df_maestro = df_maestro.merge(df_prio_keys, on='JOIN_KEY', how='left', validate='m:1')\n",
    "\n",
    "# 7) Construcción de etiqueta combinada\n",
    "#   - y_num_src: primero PROYECTO_RIESGO, si NaN usar OBRA_RIESGO (si existen)\n",
    "y_num_src = None\n",
    "if label_num_cols:\n",
    "    if 'PROYECTO_RIESGO' in label_num_cols and 'OBRA_RIESGO' in label_num_cols:\n",
    "        y_num_src = df_maestro['PROYECTO_RIESGO'].copy()\n",
    "        y_num_src = y_num_src.where(y_num_src.notna(), df_maestro['OBRA_RIESGO'])\n",
    "    else:\n",
    "        y_num_src = df_maestro[label_num_cols[0]].copy()\n",
    "\n",
    "#   - y_desc_src: primero PROYECTO_RIESGO_DESC, si NaN usar OBRA_RIESGO_DESC (si existen)\n",
    "y_desc_src = None\n",
    "if label_desc_cols:\n",
    "    if 'PROYECTO_RIESGO_DESC' in label_desc_cols and 'OBRA_RIESGO_DESC' in label_desc_cols:\n",
    "        y_desc_src = df_maestro['PROYECTO_RIESGO_DESC'].copy()\n",
    "        y_desc_src = y_desc_src.where(y_desc_src.notna(), df_maestro['OBRA_RIESGO_DESC'])\n",
    "    else:\n",
    "        y_desc_src = df_maestro[label_desc_cols[0]].copy()\n",
    "\n",
    "# 8) y_riesgo: preferir numérico; si no hay, usar descriptivo\n",
    "y = None\n",
    "if y_num_src is not None and y_num_src.notna().any():\n",
    "    y = (pd.to_numeric(y_num_src, errors='coerce').fillna(0) > 0).astype(int)\n",
    "elif y_desc_src is not None and y_desc_src.notna().any():\n",
    "    y = (y_desc_src.astype(str).str.lower().str.strip()\n",
    "         .isin(['alto','medio','riesgoso','1','si','sí','true'])).astype(int)\n",
    "\n",
    "df_maestro['y_riesgo'] = y if y is not None else 0\n",
    "\n",
    "# 9) Reporte final\n",
    "matched_num  = int(y_num_src.notna().sum()) if y_num_src is not None else 0\n",
    "matched_desc = int(y_desc_src.notna().sum()) if y_desc_src is not None else 0\n",
    "print(f\"Etiquetas numéricas traídas: {matched_num} | descriptivas traídas: {matched_desc}\")\n",
    "print('Distribución final y_riesgo:')\n",
    "print(df_maestro['y_riesgo'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Únicas JOIN_KEY Obra: 171\n",
      "Únicas JOIN_KEY Matriz: 822\n",
      "Ejemplos Obra: ['2002060' '2002210' '2015918' '2026767' '2027711']\n",
      "Ejemplos Matriz: ['2002060' '2002210' '2002604' '2015918' '2016958']\n"
     ]
    }
   ],
   "source": [
    "print(\"Únicas JOIN_KEY Obra:\", df_maestro['JOIN_KEY'].nunique())\n",
    "print(\"Únicas JOIN_KEY Matriz:\", df_prio['_JOIN_KEY'].nunique() if '_JOIN_KEY' in df_prio.columns else 'N/A')\n",
    "print(\"Ejemplos Obra:\", df_maestro['JOIN_KEY'].dropna().unique()[:5])\n",
    "print(\"Ejemplos Matriz:\", df_prio['_JOIN_KEY'].dropna().unique()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Exportar parquet y chequeos rápidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando auxiliares: 5\n",
      "Columnas antes de coalesce: 60\n",
      "Columnas después de coalesce: 60\n",
      "✅ Guardado: c:\\MaestriaUNI\\Cursos\\III-CICLO\\TesisI\\Solucion\\Deteccion_Corrupcion\\data\\processed\\dataset_obras.parquet -> (171, 53)\n",
      "Target dist:\n",
      " y_riesgo\n",
      "1    153\n",
      "0     18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Quita columnas auxiliares y residuos de merges\n",
    "aux_patterns = [\n",
    "    r'_x$', r'_y$', r'_N$', r'_D$', r'^_key$', r'^JOIN_KEY$',\n",
    "    r'^codigo_ruc_obra$', r'^codigo_dni_obra$'\n",
    "]\n",
    "drop_aux = [c for c in df_maestro.columns if any(re.search(p, c) for p in aux_patterns)]\n",
    "if drop_aux:\n",
    "    print(\"Eliminando auxiliares:\", len(drop_aux))\n",
    "    df_maestro = df_maestro.drop(columns=drop_aux, errors='ignore')\n",
    "\n",
    "# 2) Coalesce de columnas DUPLICADAS por nombre (conserva 1 columna unificada)\n",
    "def coalesce_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = df.columns\n",
    "    # agrupar índices de columnas por nombre\n",
    "    groups = cols.to_series().groupby(cols).groups\n",
    "    for name, idxs in groups.items():\n",
    "        if len(idxs) > 1:\n",
    "            # combinar de izquierda→derecha: primer no-nulo\n",
    "            block = df.iloc[:, list(idxs)]\n",
    "            combined = block.bfill(axis=1).iloc[:, 0]\n",
    "            # borrar todas las repetidas y dejar una sola con el valor combinado\n",
    "            df = df.drop(columns=block.columns)\n",
    "            df[name] = combined\n",
    "    return df\n",
    "\n",
    "print(\"Columnas antes de coalesce:\", df_maestro.shape[1])\n",
    "df_maestro = coalesce_duplicate_columns(df_maestro)\n",
    "print(\"Columnas después de coalesce:\", df_maestro.shape[1])\n",
    "\n",
    "# 3) Asegurar que 'y_riesgo' exista UNA vez y sea int {0,1}\n",
    "if 'y_riesgo' in df_maestro.columns:\n",
    "    df_maestro['y_riesgo'] = pd.to_numeric(df_maestro['y_riesgo'], errors='coerce').fillna(0).astype(int)\n",
    "else:\n",
    "    raise ValueError(\"No se encontró la columna 'y_riesgo' para exportar.\")\n",
    "\n",
    "# 4) Preparar lista final de features (evita fugas/IDs)\n",
    "cols_leak = [\n",
    "    'PROYECTO_RIESGO','PROYECTO_RIESGO_DESC','OBRA_RIESGO','OBRA_RIESGO_DESC',\n",
    "    'CODIGO_UNICO','codigo_unico','CODIGO_OBRA','IDENTIFICADOR_OBRA'\n",
    "]\n",
    "features = [c for c in df_maestro.columns if c not in cols_leak + ['y_riesgo']]\n",
    "\n",
    "# 5) Exportar parquet\n",
    "df_out = df_maestro[features + ['y_riesgo']].copy()\n",
    "df_out.to_parquet(CONFIG['output_parquet'], index=False)\n",
    "print('✅ Guardado:', CONFIG['output_parquet'], '->', df_out.shape)\n",
    "print('Target dist:\\n', df_out['y_riesgo'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
