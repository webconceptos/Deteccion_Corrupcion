{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9c98e0",
   "metadata": {},
   "source": [
    "# Sistema ML: Detección de **Riesgo de Corrupción** en Obras Públicas (Perú)\n",
    "\n",
    "Este notebook implementa un *pipeline* end-to-end para detectar obras públicas con **riesgo de corrupción**:\n",
    "1) **Ingesta** de datos (simulada si no se detectan archivos reales).  \n",
    "2) **ETL** y *feature engineering* con banderas de riesgo.  \n",
    "3) Entrenamiento de **modelos** (*baseline* y árbol de decisión/ensamble).  \n",
    "4) **Evaluación** con métricas y curvas.  \n",
    "5) **XAI**: Importancia por permutación y **PDP/ICE**.  \n",
    "6) **Exportación** de artefactos (pipeline + modelo) y función de inferencia.\n",
    "\n",
    "> ⚠️ Reemplace los *placeholders* de rutas por sus fuentes reales (SIAF, SEACE/OSCE, INFObras, Módulos CGR/BID, etc.) cuando estén disponibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b01a1",
   "metadata": {},
   "source": [
    "## Contexto y objetivo\n",
    "\n",
    "En el marco de la **Contraloría (CGR)** y el proyecto **BID‑3**, buscamos **priorizar** y **alertar** sobre obras con probabilidad de incurrir en **riesgos de corrupción** (p. ej., adicionales y ampliaciones atípicas, fraccionamiento, sanciones previas de empresas, colusión, sobrecostos, baja competencia, etc.).\n",
    "\n",
    "**Variable objetivo (label)**: `riesgo_corrupcion` (1 = alto riesgo, 0 = bajo riesgo).  \n",
    "**Fuentes típicas:** SIAF, SEACE/OSCE, INFOBRAS, PERUCOMPRAS, SERVIR, SNCP, padrones de sanciones, y registros internos de la CGR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73daf9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "sklearn: 1.7.2\n",
      "pandas : 2.3.3\n",
      "numpy  : 2.3.3\n",
      "matplot: 3.10.7\n"
     ]
    }
   ],
   "source": [
    "# == 0. Setup\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, average_precision_score,\n",
    "                             RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay, brier_score_loss)\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Ajustes visuales (sin estilos/colores específicos)\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "\n",
    "ARTIF_DIR = Path('artifacts')\n",
    "SCRIPT_DIR = Path('scripts')\n",
    "ARTIF_DIR.mkdir(exist_ok=True, parents=True)\n",
    "SCRIPT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print('Versions:')\n",
    "import sklearn, matplotlib\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('pandas :', pd.__version__)\n",
    "print('numpy  :', np.__version__)\n",
    "print('matplot:', matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac16e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron vistas previas de los diccionarios:\n",
      " - Diccionario_Datos_ML_Completo_V1.xlsx => shape: (10, 7)\n",
      " - Diccionario_Datos_Sistemas_Fuente_V1.xlsx => shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "# == 0.1 Intento de lectura de diccionarios (si existen)\n",
    "dict_files = [\n",
    "    '../data/external/catalogos/Diccionario_Datos_ML_Completo_V1.xlsx',\n",
    "    '../data/external/catalogos/Diccionario_Datos_Sistemas_Fuente_V1.xlsx'\n",
    "]\n",
    "\n",
    "loaded_dicts = {}\n",
    "for f in dict_files:\n",
    "    if os.path.exists(f):\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            loaded_dicts[os.path.basename(f)] = df.head(10)\n",
    "        except Exception as e:\n",
    "            print(f'[WARN] No se pudo leer {f}:', e)\n",
    "\n",
    "if loaded_dicts:\n",
    "    print('Se cargaron vistas previas de los diccionarios:')\n",
    "    for name, df in loaded_dicts.items():\n",
    "        print(' -', name, '=> shape:', df.shape)\n",
    "else:\n",
    "    print('No se encontraron/leyeron diccionarios en /data por ahora.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fcba2",
   "metadata": {},
   "source": [
    "## Modelo de datos (referencial)\n",
    "\n",
    "A continuación, diagramas referenciales del **Data Warehouse** para Obras, Empresas y Miembros de comité/equipo. Úselos como guía de *staging → silver → gold* y llaves de integración (CUI, RUC, N° contrato, etc.).\n",
    "\n",
    "![Matriz ML DW](/data/processed/Matriz_ML_DW.png)\n",
    "\n",
    "![Obras ML DW](/data/processed/Obras_ML_DW.png)\n",
    "\n",
    "![Empresa ML DW](/data/processed/Empresa_ML_DW.png)\n",
    "\n",
    "![Miembro ML DW](/data/processed/Miembro_ML_DW.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc1fee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Obra] shape: (0, 0)  [Empresa] shape: (0, 0)  [Func] shape: (0, 0)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No se cargó nada de 'obra' — revisa DATA_ROOT y patrones.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     41\u001b[39m func_raw = concat_many(P_FUNC, patterns=[\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDS_DASH_Miembro_*.*\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDatos generales*Funcion*.*\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m ])\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[Obra] shape:\u001b[39m\u001b[33m\"\u001b[39m, obra_raw.shape, \u001b[33m\"\u001b[39m\u001b[33m [Empresa] shape:\u001b[39m\u001b[33m\"\u001b[39m, emp_raw.shape, \u001b[33m\"\u001b[39m\u001b[33m [Func] shape:\u001b[39m\u001b[33m\"\u001b[39m, func_raw.shape)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obra_raw.empty, \u001b[33m\"\u001b[39m\u001b[33mNo se cargó nada de \u001b[39m\u001b[33m'\u001b[39m\u001b[33mobra\u001b[39m\u001b[33m'\u001b[39m\u001b[33m — revisa DATA_ROOT y patrones.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- Estandariza columnas a lo que pide el modelo ---\u001b[39;00m\n\u001b[32m     50\u001b[39m colmap_obras = {\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcosto_total\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mMontoContrato\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mCostoTotal\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcosto_total\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMonto total\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMonto_total\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplazo_meses\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mPlazoMeses\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mplazo_meses\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mPlazo (meses)\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mriesgo_corrupcion\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mriesgo_corrupcion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     64\u001b[39m }\n",
      "\u001b[31mAssertionError\u001b[39m: No se cargó nada de 'obra' — revisa DATA_ROOT y patrones."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "P_OBRA = DATA_ROOT / \"obra\"\n",
    "P_EMP  = DATA_ROOT / \"empresa\"\n",
    "P_FUNC = DATA_ROOT / \"funcionario\"\n",
    "\n",
    "def read_any(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix.lower() in [\".xlsx\", \".xls\"]:\n",
    "        # lee primera hoja; si necesitas otra, usa sheet_name=\"...\"\n",
    "        return pd.read_excel(path)\n",
    "    for enc in [\"utf-8-sig\",\"latin-1\"]:\n",
    "        try: return pd.read_csv(path, encoding=enc)\n",
    "        except Exception: pass\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def concat_many(folder: Path, patterns) -> pd.DataFrame:\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files += list(folder.glob(pat))\n",
    "    files = sorted(set(files))\n",
    "    if not files: return pd.DataFrame()\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = read_any(f)\n",
    "            df[\"_source\"] = f.name\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {f.name}: {e}\")\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "obra_raw = concat_many(P_OBRA, patterns=[\n",
    "    \"DS_DASH_Obra_*.*\",               # tus CSVs\n",
    "    \"Datos generales*Obra*.xlsx\"      # tus excels “Datos generales …”\n",
    "])\n",
    "emp_raw  = concat_many(P_EMP, patterns=[\n",
    "    \"DS_DASH_Empresa_*.*\",\n",
    "    \"Datos generales*Empresa*.xlsx\"\n",
    "])\n",
    "func_raw = concat_many(P_FUNC, patterns=[\n",
    "    \"DS_DASH_Miembro_*.*\",\n",
    "    \"Datos generales*Funcion*.*\"\n",
    "])\n",
    "\n",
    "print(\"[Obra] shape:\", obra_raw.shape, \" [Empresa] shape:\", emp_raw.shape, \" [Func] shape:\", func_raw.shape)\n",
    "assert not obra_raw.empty, \"No se cargó nada de 'obra' — revisa DATA_ROOT y patrones.\"\n",
    "\n",
    "# --- Estandariza columnas a lo que pide el modelo ---\n",
    "colmap_obras = {\n",
    "    \"costo_total\": [\"MontoContrato\",\"CostoTotal\",\"costo_total\",\"Monto total\",\"Monto_total\"],\n",
    "    \"plazo_meses\": [\"PlazoMeses\",\"plazo_meses\",\"Plazo (meses)\"],\n",
    "    \"adicionales_pct\": [\"AdicPct\",\"%Adicionales\",\"adicionales_pct\",\"Porc_Adicionales\"],\n",
    "    \"ampliaciones\": [\"NroAmpliaciones\",\"ampliaciones\"],\n",
    "    \"penalidades\": [\"NroPenalidades\",\"penalidades\"],\n",
    "    \"baja_competencia\": [\"BajaCompetencia\",\"baja_competencia\",\"PocosPostores\"],\n",
    "    \"consorcio\": [\"Consorcio\",\"consorcio\"],\n",
    "    \"experiencia_entidad\": [\"ExperienciaEntidad\",\"experiencia_entidad\"],\n",
    "    \"region_riesgo\": [\"RegionRiesgo\",\"region_riesgo\"],\n",
    "    \"tipo_proceso\": [\"TipoProceso\",\"tipo_proceso\"],\n",
    "    \"RUC\": [\"RUC\",\"ruc\"],\n",
    "    \"CUI\": [\"CUI\",\"cui\"],\n",
    "    \"riesgo_corrupcion\": [\"riesgo_corrupcion\"]\n",
    "}\n",
    "colmap_empresa = {\n",
    "    \"RUC\": [\"RUC\",\"ruc\"],\n",
    "    \"empresa_sancionada\": [\"EmpresaSancionada\",\"SancionOSCE\",\"empresa_sancionada\"]\n",
    "}\n",
    "\n",
    "def standardize(df, colmap):\n",
    "    if df.empty: return df\n",
    "    out = df.copy()\n",
    "    for std, variants in colmap.items():\n",
    "        for v in variants:\n",
    "            if v in out.columns:\n",
    "                out[std] = out[v]\n",
    "                break\n",
    "        if std not in out.columns:\n",
    "            out[std] = np.nan\n",
    "    return out\n",
    "\n",
    "obra = standardize(obra_raw, colmap_obras)\n",
    "emp  = standardize(emp_raw,  colmap_empresa)\n",
    "\n",
    "# --- Limpieza numéricos/porcentajes ---\n",
    "def to_num(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x,str):\n",
    "        x = x.replace(\"S/.\",\"\").replace(\",\",\"\").strip()\n",
    "        x = re.sub(r\"[^0-9\\.\\-eE]\", \"\", x)\n",
    "    try: return float(x)\n",
    "    except: return np.nan\n",
    "\n",
    "for c in [\"costo_total\",\"plazo_meses\",\"adicionales_pct\",\"ampliaciones\",\"penalidades\",\n",
    "          \"baja_competencia\",\"consorcio\",\"experiencia_entidad\"]:\n",
    "    if c in obra.columns: obra[c] = obra[c].map(to_num)\n",
    "\n",
    "if \"adicionales_pct\" in obra.columns:\n",
    "    obra.loc[obra[\"adicionales_pct\"]>1, \"adicionales_pct\"] = obra[\"adicionales_pct\"]/100.0\n",
    "\n",
    "def norm_tipo_proceso(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    s = str(s).lower()\n",
    "    if \"directa\" in s: return \"Contratación Directa\"\n",
    "    if \"simplificada\" in s: return \"Adjudicación Simplificada\"\n",
    "    return \"Licitación\"\n",
    "if \"tipo_proceso\" in obra.columns:\n",
    "    obra[\"tipo_proceso\"] = obra[\"tipo_proceso\"].apply(norm_tipo_proceso)\n",
    "\n",
    "def norm_region(s):\n",
    "    if pd.isna(s): return \"MEDIA\"\n",
    "    s = str(s).upper()\n",
    "    return s if s in {\"ALTA\",\"MEDIA\",\"BAJA\"} else \"MEDIA\"\n",
    "obra[\"region_riesgo\"] = obra.get(\"region_riesgo\",\"MEDIA\")\n",
    "obra[\"region_riesgo\"] = obra[\"region_riesgo\"].apply(norm_region)\n",
    "\n",
    "# empresa_sancionada por RUC\n",
    "obra[\"empresa_sancionada\"] = obra.get(\"empresa_sancionada\", np.nan)\n",
    "if not emp.empty and \"RUC\" in obra.columns and \"RUC\" in emp.columns:\n",
    "    aux = emp[[\"RUC\",\"empresa_sancionada\"]].copy()\n",
    "    if aux[\"empresa_sancionada\"].dtype == object:\n",
    "        aux[\"empresa_sancionada\"] = aux[\"empresa_sancionada\"].astype(str).str.lower().map(\n",
    "            {\"si\":1,\"sí\":1,\"true\":1,\"1\":1,\"no\":0,\"false\":0,\"0\":0}\n",
    "        ).fillna(0)\n",
    "    obra = obra.merge(aux, on=\"RUC\", how=\"left\", suffixes=(\"\",\"_emp\"))\n",
    "    obra[\"empresa_sancionada\"] = obra[\"empresa_sancionada\"].fillna(obra[\"empresa_sancionada_emp\"])\n",
    "    obra.drop(columns=[c for c in obra.columns if c.endswith(\"_emp\")], inplace=True)\n",
    "obra[\"empresa_sancionada\"] = obra[\"empresa_sancionada\"].fillna(0).astype(int)\n",
    "\n",
    "# --- Dataset final\n",
    "features_needed = [\n",
    "    \"costo_total\",\"plazo_meses\",\"adicionales_pct\",\"ampliaciones\",\"penalidades\",\n",
    "    \"baja_competencia\",\"empresa_sancionada\",\"consorcio\",\"experiencia_entidad\",\n",
    "    \"region_riesgo\",\"tipo_proceso\"\n",
    "]\n",
    "label_col = \"riesgo_corrupcion\"\n",
    "\n",
    "for c in features_needed:\n",
    "    if c not in obra.columns: obra[c] = np.nan\n",
    "\n",
    "if label_col in obra.columns:\n",
    "    obra[label_col] = pd.to_numeric(obra[label_col], errors=\"coerce\").fillna(obra[label_col]).astype(int)\n",
    "\n",
    "df = obra[features_needed + ([label_col] if label_col in obra.columns else [])].copy()\n",
    "print(\"Dataset ML listo. shape:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 2. ETL y Feature Engineering\n",
    "# Limpiezas básicas y creación de *flags* (ejemplos). Ajuste según su diccionario.\n",
    "raw = df.copy()\n",
    "\n",
    "# Ejemplo de winsorización simple para variables monetarias (evitar outliers extremos)\n",
    "def winsorize(s: pd.Series, p_low=0.01, p_high=0.99) -> pd.Series:\n",
    "    lo, hi = s.quantile(p_low), s.quantile(p_high)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "num_cols = ['costo_total', 'plazo_meses', 'adicionales_pct', 'ampliaciones', 'penalidades']\n",
    "for c in num_cols:\n",
    "    raw[c] = winsorize(raw[c])\n",
    "\n",
    "# Banderas compuestas (red flags)\n",
    "raw['flag_adicionales_altos'] = (raw['adicionales_pct'] > 0.15).astype(int)\n",
    "raw['flag_muchas_ampliaciones'] = (raw['ampliaciones'] >= 2).astype(int)\n",
    "raw['flag_penalidades'] = (raw['penalidades'] >= 1).astype(int)\n",
    "raw['flag_contratacion_directa'] = (raw['tipo_proceso'] == 'Contratación Directa').astype(int)\n",
    "raw['flag_region_alta'] = (raw['region_riesgo'] == 'ALTA').astype(int)\n",
    "\n",
    "# Definimos features/target\n",
    "target = 'riesgo_corrupcion'\n",
    "features = [c for c in raw.columns if c != target]\n",
    "\n",
    "X = raw[features].copy()\n",
    "y = raw[target].astype(int)\n",
    "\n",
    "# Columnas por tipo\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "bin_cols = [c for c in X.columns if X[c].dropna().nunique() == 2 and X[c].dtype != 'object']\n",
    "num_cols = [c for c in X.columns if c not in cat_cols + bin_cols]\n",
    "\n",
    "print('Num cols:', num_cols)\n",
    "print('Bin cols:', bin_cols)\n",
    "print('Cat cols:', cat_cols)\n",
    "\n",
    "# Pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols),\n",
    "        ('pass', 'passthrough', bin_cols)  # flags ya binarios\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print('[Split] Train:', X_train.shape, ' Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 3. Entrenamiento & Evaluación\n",
    "\n",
    "def evaluar_modelo(modelo: Pipeline, X_tr, y_tr, X_te, y_te, nombre='modelo'):\n",
    "    modelo.fit(X_tr, y_tr)\n",
    "    y_pred = modelo.predict(X_te)\n",
    "    if hasattr(modelo, 'predict_proba'):\n",
    "        y_proba = modelo.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        # algunos modelos (p. ej. SVM linear sin probas); fallback con decisión si existe\n",
    "        y_proba = getattr(modelo, 'decision_function', lambda x: y_pred)(X_te)\n",
    "        # reescalado simple si fuera necesario\n",
    "        if y_proba.ndim == 1 and (y_proba.max() > 1 or y_proba.min() < 0):\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            y_proba = MinMaxScaler().fit_transform(y_proba.reshape(-1,1)).ravel()\n",
    "\n",
    "    roc = roc_auc_score(y_te, y_proba)\n",
    "    pr  = average_precision_score(y_te, y_proba)\n",
    "\n",
    "    print(f'== {nombre} ==')\n",
    "    print('ROC-AUC :', round(roc, 4))\n",
    "    print('PR-AUC  :', round(pr, 4))\n",
    "    print('\\nClassification Report:\\n', classification_report(y_te, y_pred, digits=4))\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    RocCurveDisplay.from_predictions(y_te, y_proba, ax=ax1)\n",
    "    ax1.set_title(f'ROC — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    PrecisionRecallDisplay.from_predictions(y_te, y_proba, ax=ax2)\n",
    "    ax2.set_title(f'Precision-Recall — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    fig3, ax3 = plt.subplots()\n",
    "    ConfusionMatrixDisplay.from_predictions(y_te, y_pred, ax=ax3)\n",
    "    ax3.set_title(f'Matriz de confusión — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    # Brier score (calibración)\n",
    "    try:\n",
    "        brier = brier_score_loss(y_te, y_proba)\n",
    "        print('Brier score:', round(brier, 4))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {'roc_auc': roc, 'pr_auc': pr, 'model': modelo}\n",
    "\n",
    "# Baseline: Regresión Logística con balanceo\n",
    "baseline = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=None, solver='liblinear'))\n",
    "])\n",
    "\n",
    "res_base = evaluar_modelo(baseline, X_train, y_train, X_test, y_test, nombre='Baseline — RegLog')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest con pequeña búsqueda de hiperparámetros (rápida)\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [150, 300],\n",
    "    'rf__max_depth': [None, 8, 12],\n",
    "    'rf__min_samples_split': [2, 10],\n",
    "    'rf__min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(rf_pipe, param_grid, scoring='average_precision', cv=cv, n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Mejores params:', gs.best_params_)\n",
    "print('Mejor score (PR-AUC cv):', round(gs.best_score_, 4))\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "res_rf = evaluar_modelo(best_rf, X_train, y_train, X_test, y_test, nombre='RandomForest (mejor)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72419c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección simple del mejor modelo por PR-AUC (priorizamos alertas en clase positiva)\n",
    "best = res_rf if res_rf['pr_auc'] >= res_base['pr_auc'] else res_base\n",
    "best_name = 'RandomForest' if best is res_rf else 'RegLog'\n",
    "print('Modelo seleccionado:', best_name, '— PR-AUC:', round(best['pr_auc'], 4), ' ROC-AUC:', round(best['roc_auc'], 4))\n",
    "\n",
    "best_model = best['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 4. XAI (explicabilidad)\n",
    "# 4.1 Importancia por permutación (global)\n",
    "try:\n",
    "    pi_result = permutation_importance(best_model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "    # Recuperar nombres de features post-transformación\n",
    "    # ColumnTransformer + OneHot => necesitamos nombres expandidos\n",
    "    def get_feature_names(column_transformer, input_features):\n",
    "        # Basado en scikit-learn >=1.0\n",
    "        output = []\n",
    "        for name, trans, cols in column_transformer.transformers_:\n",
    "            if name == 'remainder' and trans == 'drop':\n",
    "                continue\n",
    "            if name == 'pass' and trans == 'passthrough':\n",
    "                # columnas pasadas tal cual\n",
    "                if cols is None:\n",
    "                    cols = [f for f in input_features if f not in sum([list(ct[2]) for ct in column_transformer.transformers_ if ct[0]!=name], [])]\n",
    "                output.extend([input_features[i] if isinstance(i, int) else i for i in cols])\n",
    "            else:\n",
    "                if hasattr(trans, 'get_feature_names_out'):\n",
    "                    # ej: 'num' StandardScaler -> no agrega sufijos; 'cat' OHE -> expande\n",
    "                    feat_names = trans.get_feature_names_out(cols)\n",
    "                    output.extend(feat_names.tolist())\n",
    "                elif trans == 'passthrough':\n",
    "                    output.extend([input_features[i] if isinstance(i, int) else i for i in cols])\n",
    "                else:\n",
    "                    # fallback\n",
    "                    output.extend([str(c) for c in cols])\n",
    "        return output\n",
    "\n",
    "    ct = best_model.named_steps['prep']\n",
    "    feat_names = get_feature_names(ct, X_test.columns)\n",
    "\n",
    "    importances = pi_result.importances_mean\n",
    "    idx = np.argsort(importances)[::-1][:20]  # top 20\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(len(idx)), importances[idx])\n",
    "    ax.set_xticks(range(len(idx)))\n",
    "    ax.set_xticklabels([feat_names[i] if i < len(feat_names) else f'f{i}' for i in idx], rotation=90)\n",
    "    ax.set_title('Importancia por permutación (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('[WARN] Falló la importancia por permutación:', e)\n",
    "\n",
    "# 4.2 PDP para 3 features numéricas más relevantes (si existen)\n",
    "num_candidates = [c for c in X_test.columns if pd.api.types.is_numeric_dtype(X_test[c])]\n",
    "top_for_pdp = num_candidates[:3]\n",
    "\n",
    "if top_for_pdp:\n",
    "    try:\n",
    "        for feat in top_for_pdp:\n",
    "            fig, ax = plt.subplots()\n",
    "            PartialDependenceDisplay.from_estimator(best_model, X_test, [feat], ax=ax)\n",
    "            ax.set_title(f'PDP — {feat}')\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('[WARN] PDP no disponible:', e)\n",
    "else:\n",
    "    print('[INFO] Sin features numéricos candidatos para PDP.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 5. Exportación de artefactos\n",
    "PIPE_PATH = ARTIF_DIR / 'preprocess_pipeline.joblib'\n",
    "MODEL_PATH = ARTIF_DIR / 'model.joblib'\n",
    "META_PATH  = ARTIF_DIR / 'metadata.json'\n",
    "\n",
    "joblib.dump(best_model.named_steps['prep'], PIPE_PATH)\n",
    "# Si el modelo es Pipeline('prep' + 'clf'), guardamos el pipeline completo para inferencia\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "\n",
    "meta = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'model': best_name,\n",
    "    'metrics': {'roc_auc': float(res_rf['roc_auc'] if best_name=='RandomForest' else res_base['roc_auc']),\n",
    "                'pr_auc': float(res_rf['pr_auc'] if best_name=='RandomForest' else res_base['pr_auc'])},\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'target': 'riesgo_corrupcion',\n",
    "    'notes': 'Reemplace datos sintéticos por sus fuentes reales.'\n",
    "}\n",
    "with open(META_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "PIPE_PATH, MODEL_PATH, META_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 6. Inferencia (función)\n",
    "def predict_from_dataframe(df_new: pd.DataFrame, model_path='artifacts/model.joblib') -> pd.DataFrame:\n",
    "    model = joblib.load(model_path)\n",
    "    # Asegurar columnas esperadas (rellenar faltantes)\n",
    "    expected = model.named_steps['prep'].get_feature_names_out if hasattr(model.named_steps['prep'], 'get_feature_names_out') else None\n",
    "    # Aplicaremos el pipeline del modelo que maneja columnas + OHE + scaler internamente\n",
    "    prob = model.predict_proba(df_new)[:, 1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    out = df_new.copy()\n",
    "    out['prob_riesgo'] = prob\n",
    "    out['pred_riesgo'] = pred\n",
    "    return out\n",
    "\n",
    "# Ejemplo mínimo con 5 filas de df (tomado del test)\n",
    "sample = X_test.head(5).copy()\n",
    "predict_from_dataframe(sample).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ed40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 7. Guardar script de inferencia por archivo (CSV)\n",
    "script_text = '''#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Uso:\n",
    "    python scripts/predict_file.py --input path/to/obras_nuevas.csv --output predicciones.csv\n",
    "\n",
    "Requiere:\n",
    "    - artifacts/model.joblib (pipeline + modelo)\n",
    "\"\"\"\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', required=True, help='Ruta del CSV de entrada')\n",
    "    parser.add_argument('--output', required=True, help='Ruta del CSV de salida')\n",
    "    parser.add_argument('--model', default='artifacts/model.joblib', help='Ruta al modelo .joblib')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.input)\n",
    "    model = joblib.load(args.model)\n",
    "\n",
    "    prob = model.predict_proba(df)[:, 1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out['prob_riesgo'] = prob\n",
    "    df_out['pred_riesgo'] = pred\n",
    "\n",
    "    df_out.to_csv(args.output, index=False)\n",
    "    print(f'[OK] Predicciones guardadas en: {args.output}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "(SCRIPT_DIR / 'predict_file.py').write_text(script_text, encoding='utf-8')\n",
    "print('[OK] scripts/predict_file.py creado.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
