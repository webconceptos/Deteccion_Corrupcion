{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9c98e0",
   "metadata": {},
   "source": [
    "# Sistema ML: Detección de **Riesgo de Corrupción** en Obras Públicas (Perú)\n",
    "\n",
    "Este notebook implementa un *pipeline* end-to-end para detectar obras públicas con **riesgo de corrupción**:\n",
    "1) **Ingesta** de datos (simulada si no se detectan archivos reales).  \n",
    "2) **ETL** y *feature engineering* con banderas de riesgo.  \n",
    "3) Entrenamiento de **modelos** (*baseline* y árbol de decisión/ensamble).  \n",
    "4) **Evaluación** con métricas y curvas.  \n",
    "5) **XAI**: Importancia por permutación y **PDP/ICE**.  \n",
    "6) **Exportación** de artefactos (pipeline + modelo) y función de inferencia.\n",
    "\n",
    "> ⚠️ Reemplace los *placeholders* de rutas por sus fuentes reales (SIAF, SEACE/OSCE, INFObras, Módulos CGR/BID, etc.) cuando estén disponibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b01a1",
   "metadata": {},
   "source": [
    "## Contexto y objetivo\n",
    "\n",
    "En el marco de la **Contraloría (CGR)** y el proyecto **BID‑3**, buscamos **priorizar** y **alertar** sobre obras con probabilidad de incurrir en **riesgos de corrupción** (p. ej., adicionales y ampliaciones atípicas, fraccionamiento, sanciones previas de empresas, colusión, sobrecostos, baja competencia, etc.).\n",
    "\n",
    "**Variable objetivo (label)**: `riesgo_corrupcion` (1 = alto riesgo, 0 = bajo riesgo).  \n",
    "**Fuentes típicas:** SIAF, SEACE/OSCE, INFOBRAS, PERUCOMPRAS, SERVIR, SNCP, padrones de sanciones, y registros internos de la CGR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73daf9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "sklearn: 1.7.2\n",
      "pandas : 2.3.3\n",
      "numpy  : 2.3.3\n",
      "matplot: 3.10.7\n"
     ]
    }
   ],
   "source": [
    "# == 0. Setup\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, average_precision_score,\n",
    "                             RocCurveDisplay, PrecisionRecallDisplay, ConfusionMatrixDisplay, brier_score_loss)\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Ajustes visuales (sin estilos/colores específicos)\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "\n",
    "ARTIF_DIR = Path('artifacts')\n",
    "SCRIPT_DIR = Path('scripts')\n",
    "ARTIF_DIR.mkdir(exist_ok=True, parents=True)\n",
    "SCRIPT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print('Versions:')\n",
    "import sklearn, matplotlib\n",
    "print('sklearn:', sklearn.__version__)\n",
    "print('pandas :', pd.__version__)\n",
    "print('numpy  :', np.__version__)\n",
    "print('matplot:', matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac16e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se cargaron vistas previas de los diccionarios:\n",
      " - Diccionario_Datos_ML_Completo_V1.xlsx => shape: (10, 7)\n",
      " - Diccionario_Datos_Sistemas_Fuente_V1.xlsx => shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "# == 0.1 Intento de lectura de diccionarios (si existen)\n",
    "dict_files = [\n",
    "    '../data/external/catalogos/Diccionario_Datos_ML_Completo_V1.xlsx',\n",
    "    '../data/external/catalogos/Diccionario_Datos_Sistemas_Fuente_V1.xlsx'\n",
    "]\n",
    "\n",
    "loaded_dicts = {}\n",
    "for f in dict_files:\n",
    "    if os.path.exists(f):\n",
    "        try:\n",
    "            df = pd.read_excel(f)\n",
    "            loaded_dicts[os.path.basename(f)] = df.head(10)\n",
    "        except Exception as e:\n",
    "            print(f'[WARN] No se pudo leer {f}:', e)\n",
    "\n",
    "if loaded_dicts:\n",
    "    print('Se cargaron vistas previas de los diccionarios:')\n",
    "    for name, df in loaded_dicts.items():\n",
    "        print(' -', name, '=> shape:', df.shape)\n",
    "else:\n",
    "    print('No se encontraron/leyeron diccionarios en /data por ahora.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fcba2",
   "metadata": {},
   "source": [
    "## Modelo de datos (referencial)\n",
    "\n",
    "A continuación, diagramas referenciales del **Data Warehouse** para Obras, Empresas y Miembros de comité/equipo. Úselos como guía de *staging → silver → gold* y llaves de integración (CUI, RUC, N° contrato, etc.).\n",
    "\n",
    "![Matriz ML DW](/data/processed/Matriz_ML_DW.png)\n",
    "\n",
    "![Obras ML DW](/data/processed/Obras_ML_DW.png)\n",
    "\n",
    "![Empresa ML DW](/data/processed/Empresa_ML_DW.png)\n",
    "\n",
    "![Miembro ML DW](/data/processed/Miembro_ML_DW.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 1. Ingesta\n",
    "# Reemplace estas rutas por sus fuentes reales cuando estén listas.\n",
    "path_obras = '/mnt/data/obras.csv'        # ejemplo\n",
    "path_empresas = '/mnt/data/empresas.csv'  # ejemplo\n",
    "path_func = '/mnt/data/funcionarios.csv'  # ejemplo\n",
    "\n",
    "def generar_datos_sinteticos(n=3000, seed=42) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Variables numéricas simuladas\n",
    "    costo = rng.lognormal(mean=15, sigma=0.6, size=n)  # ~ S/ millones (escala logn)\n",
    "    plazo_meses = rng.integers(4, 60, size=n)\n",
    "    adicionales_pct = np.clip(rng.normal(0.08, 0.07, n), 0, 0.6)  # % del monto\n",
    "    ampliaciones = np.clip(rng.poisson(0.8, n), 0, 10)\n",
    "    penalidades = np.clip(rng.poisson(0.2, n), 0, 5)\n",
    "    baja_competencia = rng.integers(0, 2, n)  # 1 si hubo poca competencia (p. ej., 1-2 postores)\n",
    "    empresa_sancionada = rng.integers(0, 2, n)\n",
    "    consorcio = rng.integers(0, 2, n)\n",
    "    experiencia_entidad = rng.integers(0, 2, n)  # 1 si la entidad tiene historial con el proveedor\n",
    "    region_riesgo = rng.choice(['ALTA', 'MEDIA', 'BAJA'], size=n, p=[0.3, 0.5, 0.2])\n",
    "    tipo_proceso = rng.choice(['Licitación', 'Adjudicación Simplificada', 'Contratación Directa'], size=n, p=[0.55, 0.35, 0.10])\n",
    "\n",
    "    # Riesgo (label) con relación no lineal y ruido\n",
    "    score = (\n",
    "        0.5*(adicionales_pct > 0.15)\n",
    "        + 0.4*(ampliaciones >= 2)\n",
    "        + 0.3*empresa_sancionada\n",
    "        + 0.25*baja_competencia\n",
    "        + 0.2*(penalidades >= 1)\n",
    "        + 0.15*(tipo_proceso == 'Contratación Directa')\n",
    "        + 0.1*(region_riesgo == 'ALTA')\n",
    "        + 0.05*(consorcio == 1)\n",
    "    )\n",
    "    prob = 1 / (1 + np.exp(-(score + rng.normal(0, 0.3, n))))\n",
    "    riesgo = (prob > 0.55).astype(int)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'costo_total': costo,\n",
    "        'plazo_meses': plazo_meses,\n",
    "        'adicionales_pct': adicionales_pct,\n",
    "        'ampliaciones': ampliaciones,\n",
    "        'penalidades': penalidades,\n",
    "        'baja_competencia': baja_competencia,\n",
    "        'empresa_sancionada': empresa_sancionada,\n",
    "        'consorcio': consorcio,\n",
    "        'experiencia_entidad': experiencia_entidad,\n",
    "        'region_riesgo': region_riesgo,\n",
    "        'tipo_proceso': tipo_proceso,\n",
    "        'riesgo_corrupcion': riesgo\n",
    "    })\n",
    "    return df\n",
    "\n",
    "if not (os.path.exists(path_obras) and os.path.exists(path_empresas) and os.path.exists(path_func)):\n",
    "    print('[INFO] No se hallaron archivos reales → generando **datos sintéticos**.')\n",
    "    df = generar_datos_sinteticos(n=3000, seed=42)\n",
    "else:\n",
    "    # Ejemplo de lectura e integración (ajuste columnas/llaves reales)\n",
    "    obras = pd.read_csv(path_obras)\n",
    "    empresas = pd.read_csv(path_empresas)\n",
    "    func = pd.read_csv(path_func)\n",
    "\n",
    "    # Integración: suponga llaves CUI, RUC, etc. (placeholder)\n",
    "    df = obras.merge(empresas, how='left', on='RUC')\\\n",
    "              .merge(func, how='left', on='CUI')\n",
    "\n",
    "print('Shape dataset:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 2. ETL y Feature Engineering\n",
    "# Limpiezas básicas y creación de *flags* (ejemplos). Ajuste según su diccionario.\n",
    "raw = df.copy()\n",
    "\n",
    "# Ejemplo de winsorización simple para variables monetarias (evitar outliers extremos)\n",
    "def winsorize(s: pd.Series, p_low=0.01, p_high=0.99) -> pd.Series:\n",
    "    lo, hi = s.quantile(p_low), s.quantile(p_high)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "num_cols = ['costo_total', 'plazo_meses', 'adicionales_pct', 'ampliaciones', 'penalidades']\n",
    "for c in num_cols:\n",
    "    raw[c] = winsorize(raw[c])\n",
    "\n",
    "# Banderas compuestas (red flags)\n",
    "raw['flag_adicionales_altos'] = (raw['adicionales_pct'] > 0.15).astype(int)\n",
    "raw['flag_muchas_ampliaciones'] = (raw['ampliaciones'] >= 2).astype(int)\n",
    "raw['flag_penalidades'] = (raw['penalidades'] >= 1).astype(int)\n",
    "raw['flag_contratacion_directa'] = (raw['tipo_proceso'] == 'Contratación Directa').astype(int)\n",
    "raw['flag_region_alta'] = (raw['region_riesgo'] == 'ALTA').astype(int)\n",
    "\n",
    "# Definimos features/target\n",
    "target = 'riesgo_corrupcion'\n",
    "features = [c for c in raw.columns if c != target]\n",
    "\n",
    "X = raw[features].copy()\n",
    "y = raw[target].astype(int)\n",
    "\n",
    "# Columnas por tipo\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "bin_cols = [c for c in X.columns if X[c].dropna().nunique() == 2 and X[c].dtype != 'object']\n",
    "num_cols = [c for c in X.columns if c not in cat_cols + bin_cols]\n",
    "\n",
    "print('Num cols:', num_cols)\n",
    "print('Bin cols:', bin_cols)\n",
    "print('Cat cols:', cat_cols)\n",
    "\n",
    "# Pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols),\n",
    "        ('pass', 'passthrough', bin_cols)  # flags ya binarios\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print('[Split] Train:', X_train.shape, ' Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 3. Entrenamiento & Evaluación\n",
    "\n",
    "def evaluar_modelo(modelo: Pipeline, X_tr, y_tr, X_te, y_te, nombre='modelo'):\n",
    "    modelo.fit(X_tr, y_tr)\n",
    "    y_pred = modelo.predict(X_te)\n",
    "    if hasattr(modelo, 'predict_proba'):\n",
    "        y_proba = modelo.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        # algunos modelos (p. ej. SVM linear sin probas); fallback con decisión si existe\n",
    "        y_proba = getattr(modelo, 'decision_function', lambda x: y_pred)(X_te)\n",
    "        # reescalado simple si fuera necesario\n",
    "        if y_proba.ndim == 1 and (y_proba.max() > 1 or y_proba.min() < 0):\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            y_proba = MinMaxScaler().fit_transform(y_proba.reshape(-1,1)).ravel()\n",
    "\n",
    "    roc = roc_auc_score(y_te, y_proba)\n",
    "    pr  = average_precision_score(y_te, y_proba)\n",
    "\n",
    "    print(f'== {nombre} ==')\n",
    "    print('ROC-AUC :', round(roc, 4))\n",
    "    print('PR-AUC  :', round(pr, 4))\n",
    "    print('\\nClassification Report:\\n', classification_report(y_te, y_pred, digits=4))\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    RocCurveDisplay.from_predictions(y_te, y_proba, ax=ax1)\n",
    "    ax1.set_title(f'ROC — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    PrecisionRecallDisplay.from_predictions(y_te, y_proba, ax=ax2)\n",
    "    ax2.set_title(f'Precision-Recall — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    fig3, ax3 = plt.subplots()\n",
    "    ConfusionMatrixDisplay.from_predictions(y_te, y_pred, ax=ax3)\n",
    "    ax3.set_title(f'Matriz de confusión — {nombre}')\n",
    "    plt.show()\n",
    "\n",
    "    # Brier score (calibración)\n",
    "    try:\n",
    "        brier = brier_score_loss(y_te, y_proba)\n",
    "        print('Brier score:', round(brier, 4))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {'roc_auc': roc, 'pr_auc': pr, 'model': modelo}\n",
    "\n",
    "# Baseline: Regresión Logística con balanceo\n",
    "baseline = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=None, solver='liblinear'))\n",
    "])\n",
    "\n",
    "res_base = evaluar_modelo(baseline, X_train, y_train, X_test, y_test, nombre='Baseline — RegLog')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest con pequeña búsqueda de hiperparámetros (rápida)\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('prep', preprocess),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [150, 300],\n",
    "    'rf__max_depth': [None, 8, 12],\n",
    "    'rf__min_samples_split': [2, 10],\n",
    "    'rf__min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(rf_pipe, param_grid, scoring='average_precision', cv=cv, n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Mejores params:', gs.best_params_)\n",
    "print('Mejor score (PR-AUC cv):', round(gs.best_score_, 4))\n",
    "\n",
    "best_rf = gs.best_estimator_\n",
    "res_rf = evaluar_modelo(best_rf, X_train, y_train, X_test, y_test, nombre='RandomForest (mejor)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72419c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección simple del mejor modelo por PR-AUC (priorizamos alertas en clase positiva)\n",
    "best = res_rf if res_rf['pr_auc'] >= res_base['pr_auc'] else res_base\n",
    "best_name = 'RandomForest' if best is res_rf else 'RegLog'\n",
    "print('Modelo seleccionado:', best_name, '— PR-AUC:', round(best['pr_auc'], 4), ' ROC-AUC:', round(best['roc_auc'], 4))\n",
    "\n",
    "best_model = best['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 4. XAI (explicabilidad)\n",
    "# 4.1 Importancia por permutación (global)\n",
    "try:\n",
    "    pi_result = permutation_importance(best_model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "    # Recuperar nombres de features post-transformación\n",
    "    # ColumnTransformer + OneHot => necesitamos nombres expandidos\n",
    "    def get_feature_names(column_transformer, input_features):\n",
    "        # Basado en scikit-learn >=1.0\n",
    "        output = []\n",
    "        for name, trans, cols in column_transformer.transformers_:\n",
    "            if name == 'remainder' and trans == 'drop':\n",
    "                continue\n",
    "            if name == 'pass' and trans == 'passthrough':\n",
    "                # columnas pasadas tal cual\n",
    "                if cols is None:\n",
    "                    cols = [f for f in input_features if f not in sum([list(ct[2]) for ct in column_transformer.transformers_ if ct[0]!=name], [])]\n",
    "                output.extend([input_features[i] if isinstance(i, int) else i for i in cols])\n",
    "            else:\n",
    "                if hasattr(trans, 'get_feature_names_out'):\n",
    "                    # ej: 'num' StandardScaler -> no agrega sufijos; 'cat' OHE -> expande\n",
    "                    feat_names = trans.get_feature_names_out(cols)\n",
    "                    output.extend(feat_names.tolist())\n",
    "                elif trans == 'passthrough':\n",
    "                    output.extend([input_features[i] if isinstance(i, int) else i for i in cols])\n",
    "                else:\n",
    "                    # fallback\n",
    "                    output.extend([str(c) for c in cols])\n",
    "        return output\n",
    "\n",
    "    ct = best_model.named_steps['prep']\n",
    "    feat_names = get_feature_names(ct, X_test.columns)\n",
    "\n",
    "    importances = pi_result.importances_mean\n",
    "    idx = np.argsort(importances)[::-1][:20]  # top 20\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(len(idx)), importances[idx])\n",
    "    ax.set_xticks(range(len(idx)))\n",
    "    ax.set_xticklabels([feat_names[i] if i < len(feat_names) else f'f{i}' for i in idx], rotation=90)\n",
    "    ax.set_title('Importancia por permutación (Top 20)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('[WARN] Falló la importancia por permutación:', e)\n",
    "\n",
    "# 4.2 PDP para 3 features numéricas más relevantes (si existen)\n",
    "num_candidates = [c for c in X_test.columns if pd.api.types.is_numeric_dtype(X_test[c])]\n",
    "top_for_pdp = num_candidates[:3]\n",
    "\n",
    "if top_for_pdp:\n",
    "    try:\n",
    "        for feat in top_for_pdp:\n",
    "            fig, ax = plt.subplots()\n",
    "            PartialDependenceDisplay.from_estimator(best_model, X_test, [feat], ax=ax)\n",
    "            ax.set_title(f'PDP — {feat}')\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('[WARN] PDP no disponible:', e)\n",
    "else:\n",
    "    print('[INFO] Sin features numéricos candidatos para PDP.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 5. Exportación de artefactos\n",
    "PIPE_PATH = ARTIF_DIR / 'preprocess_pipeline.joblib'\n",
    "MODEL_PATH = ARTIF_DIR / 'model.joblib'\n",
    "META_PATH  = ARTIF_DIR / 'metadata.json'\n",
    "\n",
    "joblib.dump(best_model.named_steps['prep'], PIPE_PATH)\n",
    "# Si el modelo es Pipeline('prep' + 'clf'), guardamos el pipeline completo para inferencia\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "\n",
    "meta = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'model': best_name,\n",
    "    'metrics': {'roc_auc': float(res_rf['roc_auc'] if best_name=='RandomForest' else res_base['roc_auc']),\n",
    "                'pr_auc': float(res_rf['pr_auc'] if best_name=='RandomForest' else res_base['pr_auc'])},\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'target': 'riesgo_corrupcion',\n",
    "    'notes': 'Reemplace datos sintéticos por sus fuentes reales.'\n",
    "}\n",
    "with open(META_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "PIPE_PATH, MODEL_PATH, META_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da2fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 6. Inferencia (función)\n",
    "def predict_from_dataframe(df_new: pd.DataFrame, model_path='artifacts/model.joblib') -> pd.DataFrame:\n",
    "    model = joblib.load(model_path)\n",
    "    # Asegurar columnas esperadas (rellenar faltantes)\n",
    "    expected = model.named_steps['prep'].get_feature_names_out if hasattr(model.named_steps['prep'], 'get_feature_names_out') else None\n",
    "    # Aplicaremos el pipeline del modelo que maneja columnas + OHE + scaler internamente\n",
    "    prob = model.predict_proba(df_new)[:, 1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "    out = df_new.copy()\n",
    "    out['prob_riesgo'] = prob\n",
    "    out['pred_riesgo'] = pred\n",
    "    return out\n",
    "\n",
    "# Ejemplo mínimo con 5 filas de df (tomado del test)\n",
    "sample = X_test.head(5).copy()\n",
    "predict_from_dataframe(sample).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ed40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 7. Guardar script de inferencia por archivo (CSV)\n",
    "script_text = '''#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Uso:\n",
    "    python scripts/predict_file.py --input path/to/obras_nuevas.csv --output predicciones.csv\n",
    "\n",
    "Requiere:\n",
    "    - artifacts/model.joblib (pipeline + modelo)\n",
    "\"\"\"\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', required=True, help='Ruta del CSV de entrada')\n",
    "    parser.add_argument('--output', required=True, help='Ruta del CSV de salida')\n",
    "    parser.add_argument('--model', default='artifacts/model.joblib', help='Ruta al modelo .joblib')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    df = pd.read_csv(args.input)\n",
    "    model = joblib.load(args.model)\n",
    "\n",
    "    prob = model.predict_proba(df)[:, 1]\n",
    "    pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out['prob_riesgo'] = prob\n",
    "    df_out['pred_riesgo'] = pred\n",
    "\n",
    "    df_out.to_csv(args.output, index=False)\n",
    "    print(f'[OK] Predicciones guardadas en: {args.output}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "(SCRIPT_DIR / 'predict_file.py').write_text(script_text, encoding='utf-8')\n",
    "print('[OK] scripts/predict_file.py creado.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
